#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»PLINK2é€šè¿‡VCFä¸­é—´æ ¼å¼ç”Ÿæˆæ ·æœ¬ä¸€è‡´æ€§åºåˆ—
é¿å…pgenlibçš„segfaulté—®é¢˜
"""

import os
import json
import subprocess
import pandas as pd
import pysam
from tqdm import tqdm
import tempfile
import shutil

# =====================
# è·¯å¾„é…ç½®
# =====================
bed_file = "GAD1.bed"
pheno_file = "/mnt/zzb/default/Workspace/Rice-Genome/application/GWAS_fine_mapping/RiceGWAScohort/phenotyping_data/2_3K_Rice_pheno"
pgen_prefix = "RICE_RP_GAD1"
fasta_file = "/mnt/zzb/default/Public/OsGenos/Oryza_sativa/chromosome/GCA_001433935.1_IRGSP-1.0_genomic.fna.gz"

target_pheno_col = "awn_presence"
out_dir = "json_blocks_APTrue_with_indel_vcf"
os.makedirs(out_dir, exist_ok=True)

# åˆ›å»ºä¸´æ—¶ç›®å½•
temp_dir = tempfile.mkdtemp(prefix="plink2_vcf_")
print(f"ä¸´æ—¶ç›®å½•: {temp_dir}")

# =====================
# æŸ“è‰²ä½“æ˜ å°„
# =====================
chrom_map = {
    '1': 'AP014957.1', '2': 'AP014958.1', '3': 'AP014959.1',
    '4': 'AP014960.1', '5': 'AP014961.1', '6': 'AP014962.1',
    '7': 'AP014963.1', '8': 'AP014964.1', '9': 'AP014965.1',
    '10': 'AP014966.1', '11': 'AP014967.1', '12': 'AP014968.1'
}

# =====================
# 1. è¯»å–è¡¨å‹
# =====================
print("â¡ï¸ è¯»å–è¡¨å‹æ•°æ® ...")
pheno = pd.read_csv(pheno_file, sep="\t")
pheno = pheno.dropna(subset=[target_pheno_col])
pheno = pheno.set_index("SampleID")
print(f"âœ… æœ‰æ•ˆè¡¨å‹æ ·æœ¬æ•°: {len(pheno)}")

# =====================
# 2. è¯»å–BEDåŒºé—´
# =====================
print("â¡ï¸ è¯»å–BEDæ–‡ä»¶ ...")
bed_df = pd.read_csv(bed_file, sep="\t", header=None, names=["chrom", "start", "end"])
print(f"âœ… BED åŒºé—´æ•°: {len(bed_df)}")

# =====================
# 3. è¯»å–FASTAå‚è€ƒåºåˆ—
# =====================
print("â¡ï¸ æ‰“å¼€FASTAæ–‡ä»¶ ...")
fasta = pysam.FastaFile(fasta_file)

# =====================
# 4. è¯»å–æ ·æœ¬IDæ˜ å°„
# =====================
print("â¡ï¸ è¯»å–PSAMè·å–æ ·æœ¬åˆ—è¡¨ ...")
psam = pd.read_csv(f"{pgen_prefix}.psam", sep="\t")
psam.columns = psam.columns.str.replace('#', '')

# æ‰¾åˆ°æ ·æœ¬IDåˆ—
sample_col = None
for col in ['IID', 'iid']:
    if col in psam.columns:
        sample_col = col
        break
if sample_col is None:
    sample_col = psam.columns[1] if len(psam.columns) >= 2 else psam.columns[0]

all_sample_ids = psam[sample_col].tolist()
print(f"âœ… åŸºå› å‹æ ·æœ¬æ•°: {len(all_sample_ids)}")

# æ ·æœ¬åŒ¹é…
pheno_samples = [s for s in pheno.index if s in all_sample_ids]
sample_to_idx = {sid: i for i, sid in enumerate(pheno_samples)}
labels = pheno.loc[pheno_samples, target_pheno_col].values

print(f"âœ… å…±åŒæ ·æœ¬æ•°: {len(pheno_samples)}")

if len(pheno_samples) == 0:
    print("âŒ é”™è¯¯: æ²¡æœ‰å…±åŒæ ·æœ¬")
    exit(1)

# =====================
# 5. è¾…åŠ©å‡½æ•°
# =====================
def classify_variant(ref, alt):
    """åˆ¤æ–­å˜å¼‚ç±»å‹"""
    ref = str(ref).upper()
    alt = str(alt).upper()
    
    len_ref = len(ref)
    len_alt = len(alt)
    
    if len_ref == 1 and len_alt == 1:
        return 'SNP'
    elif len_ref < len_alt:
        return 'INS'
    elif len_ref > len_alt:
        return 'DEL'
    else:
        return 'COMPLEX'


def apply_variant_to_sequence(ref_seq, variant_list):
    """å°†å˜å¼‚åº”ç”¨åˆ°å‚è€ƒåºåˆ—"""
    variant_list = sorted(variant_list, key=lambda x: x[0], reverse=True)
    seq = list(ref_seq)
    
    for offset, var_type, ref, alt in variant_list:
        if offset < 0 or offset >= len(seq):
            continue
        
        if var_type == 'SNP':
            seq[offset] = alt
            
        elif var_type == 'INS':
            seq[offset] = alt[0]
            insert_bases = alt[1:]
            for i, base in enumerate(insert_bases):
                seq.insert(offset + 1 + i, base)
            
        elif var_type == 'DEL':
            seq[offset] = alt[0] if len(alt) > 0 else 'N'
            for i in range(1, len(ref)):
                if offset + i < len(seq):
                    seq[offset + i] = 'N'
        
        elif var_type == 'COMPLEX':
            seq[offset] = alt[0] if len(alt) > 0 else 'N'
            if len(ref) > 1:
                for i in range(1, len(ref)):
                    if offset + i < len(seq):
                        seq[offset + i] = 'N'
    
    return ''.join(seq)


def extract_region_to_vcf(pgen_prefix, chrom, start, end, output_vcf):
    """
    ä½¿ç”¨PLINK2å°†æŒ‡å®šåŒºé—´å¯¼å‡ºä¸ºVCF
    """
    cmd = [
        'plink2',
        '--pfile', pgen_prefix,
        '--chr', str(chrom),
        '--from-bp', str(start + 1),
        '--to-bp', str(end),
        '--export', 'vcf',
        '--out', output_vcf.replace('.vcf', '').replace('.vcf.gz', '')
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        # PLINK2è¾“å‡ºçš„æ˜¯ xxx.vcf
        expected_vcf = output_vcf.replace('.vcf', '').replace('.vcf.gz', '') + '.vcf'
        
        if os.path.exists(expected_vcf):
            # å¦‚æœéœ€è¦å‹ç¼©
            if output_vcf.endswith('.gz'):
                subprocess.run(['bgzip', '-f', expected_vcf], check=True)
                subprocess.run(['tabix', '-f', '-p', 'vcf', expected_vcf + '.gz'], check=True)
                return expected_vcf + '.gz'
            return expected_vcf
        else:
            print(f"  âš ï¸  VCFæ–‡ä»¶æœªç”Ÿæˆ: {expected_vcf}")
            return None
    except subprocess.CalledProcessError as e:
        print(f"  âœ— PLINK2é”™è¯¯: {e.stderr}")
        return None
    except FileNotFoundError:
        print("  âœ— é”™è¯¯: æœªæ‰¾åˆ°plink2å‘½ä»¤")
        print("    è¯·å®‰è£…: conda install -c bioconda plink2")
        exit(1)


# =====================
# 6. ä¸»å¾ªç¯
# =====================
print("\n" + "="*60)
print("å¼€å§‹å¤„ç†blocks...")
print("="*60)

for block_id, row in tqdm(bed_df.iterrows(), total=len(bed_df), desc="Processing blocks"):
    chrom = str(row['chrom'])
    start = int(row['start'])
    end = int(row['end'])
    block_name = f"block_{block_id+1}"
    
    if chrom not in chrom_map:
        print(f"âš ï¸  è·³è¿‡ {block_name}: æŸ“è‰²ä½“ {chrom} ä¸åœ¨æ˜ å°„è¡¨ä¸­")
        continue
    
    # æå–å‚è€ƒåºåˆ—
    try:
        ref_seq = fasta.fetch(chrom_map[chrom], start, end).upper()
    except Exception as e:
        print(f"âš ï¸  è·³è¿‡ {block_name}: FASTAæå–å¤±è´¥ - {e}")
        continue
    
    print(f"\nğŸ“ {block_name}: chr{chrom}:{start}-{end}")
    
    # å¯¼å‡ºè¯¥åŒºé—´çš„VCF
    vcf_path = os.path.join(temp_dir, f"{block_name}.vcf")
    vcf_file = extract_region_to_vcf(pgen_prefix, chrom, start, end, vcf_path)
    
    if vcf_file is None or not os.path.exists(vcf_file):
        print(f"  âš ï¸  è·³è¿‡ {block_name}: VCFå¯¼å‡ºå¤±è´¥")
        continue
    
    # è¯»å–VCF
    try:
        vcf = pysam.VariantFile(vcf_file)
    except Exception as e:
        print(f"  âœ— VCFè¯»å–å¤±è´¥: {e}")
        continue
    
    # è·å–VCFä¸­çš„æ ·æœ¬åˆ—è¡¨
    vcf_samples = list(vcf.header.samples)
    
    # åŒ¹é…æ ·æœ¬
    common_in_vcf = [s for s in pheno_samples if s in vcf_samples]
    
    if len(common_in_vcf) == 0:
        print(f"  âš ï¸  è·³è¿‡ {block_name}: VCFä¸­æ— å…±åŒæ ·æœ¬")
        vcf.close()
        continue
    
    print(f"  VCFæ ·æœ¬æ•°: {len(vcf_samples)}, å…±åŒæ ·æœ¬: {len(common_in_vcf)}")
    
    # æ”¶é›†è¯¥åŒºé—´çš„æ‰€æœ‰å˜å¼‚
    variants_list = []
    for record in vcf.fetch():
        variants_list.append(record)
    
    print(f"  å˜å¼‚æ•°: {len(variants_list)}")
    
    if len(variants_list) == 0:
        vcf.close()
        continue
    
    # é€æ ·æœ¬ç”Ÿæˆåºåˆ—
    json_list = []
    
    for sample in tqdm(common_in_vcf, desc=f"  {block_name} æ ·æœ¬", leave=False):
        sample_variants = []
        
        for record in variants_list:
            pos = record.pos
            ref = str(record.ref).upper()
            alts = [str(a).upper() for a in record.alts] if record.alts else []
            
            # è¿‡æ»¤ç‰¹æ®Šæ ‡è®°
            alts = [a for a in alts if a not in ['*', '<DEL>', 'DEL']]
            
            if len(alts) == 0:
                continue
            
            # è·å–è¯¥æ ·æœ¬çš„åŸºå› å‹
            try:
                gt = record.samples[sample]['GT']
            except:
                gt = (None, None)
            
            # è§£æåŸºå› å‹
            if gt is None or None in gt:
                # Missing
                applied_allele = 'N' * len(ref)
            elif gt == (0, 0):
                # REF/REF
                continue
            elif 0 in gt:
                # æ‚åˆ: ä½¿ç”¨ALT
                alt_idx = max(gt) - 1
                applied_allele = alts[alt_idx] if alt_idx < len(alts) else alts[0]
            else:
                # çº¯åˆALT
                alt_idx = gt[0] - 1
                applied_allele = alts[alt_idx] if alt_idx < len(alts) else alts[0]
            
            # è®¡ç®—offset
            offset = pos - start - 1
            
            # å˜å¼‚ç±»å‹
            var_type = classify_variant(ref, applied_allele)
            
            sample_variants.append((offset, var_type, ref, applied_allele))
        
        # ç”Ÿæˆåºåˆ—
        try:
            consensus_seq = apply_variant_to_sequence(ref_seq, sample_variants)
        except Exception as e:
            print(f"\n  âœ— æ ·æœ¬ {sample} å¤±è´¥: {e}")
            consensus_seq = ref_seq
        
        # è·å–æ ‡ç­¾
        label = int(pheno.loc[sample, target_pheno_col])
        
        json_list.append({
            "label": label,
            "spec": sample,
            "loc": block_name,
            "sequence": consensus_seq
        })
    
    vcf.close()
    
    # ä¿å­˜JSON
    out_path = os.path.join(out_dir, f"{block_name}.json")
    with open(out_path, "w") as f:
        json.dump(json_list, f, indent=2)
    
    print(f"  âœ… {block_name} å®Œæˆ | æ ·æœ¬æ•°={len(json_list)}")

# =====================
# 7. æ¸…ç†
# =====================
fasta.close()

print("\næ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
shutil.rmtree(temp_dir)

print("\n" + "="*60)
print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
print(f"è¾“å‡ºç›®å½•: {out_dir}")
print("="*60)
