#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµå¼æ³¨æ„åŠ›åˆ†æ•°æå–ï¼ˆå†…å­˜é«˜æ•ˆç‰ˆï¼‰
åŸºäºQ/Kæ‰‹åŠ¨è®¡ç®—ï¼Œæ”¯æŒè¶…é•¿åºåˆ—
"""

import argparse
import os
import json
import math
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm

import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

# ==================== æ ¸å¿ƒå‡½æ•°ï¼ˆæ¥è‡ªæ–‡æ¡£6ï¼‰====================

captured: Dict[str, torch.Tensor] = {}

@torch.no_grad()
def attn_column_sums_streaming(q, k, causal=True, block_cols=1024):
    """
    è¿”å› [B, L]ï¼šæ‰€æœ‰å¤´å¹³å‡åçš„ attention çŸ©é˜µæŒ‰åˆ—æ±‚å’Œï¼ˆä¸æ„é€  LxLï¼‰ã€‚
    q,k: [B,H,L,D]
    """
    B, H, L, D = q.shape
    scale = 1.0 / math.sqrt(D)
    out = q.new_zeros((B, L), dtype=torch.float32)

    for b in range(B):
        for h in range(H):
            Q = q[b, h].to(torch.float32)          # [L, D]
            K = k[b, h].to(torch.float32)          # [L, D]

            m = torch.full((L,), -float('inf'), dtype=torch.float32, device=Q.device)
            l = torch.zeros((L,), dtype=torch.float32, device=Q.device)
            col_sum = torch.zeros((L,), dtype=torch.float32, device=Q.device)

            for j0 in range(0, L, block_cols):
                j1 = min(j0 + block_cols, L)
                Kb = K[j0:j1]                                  # [B,D]
                S = (Q @ Kb.t()) * scale                       # [L,B]

                if causal:
                    row_idx = torch.arange(L, device=Q.device).unsqueeze(1)
                    col_idx = torch.arange(j0, j1, device=Q.device).unsqueeze(0)
                    S = S.masked_fill(col_idx > row_idx, float('-inf'))

                block_row_max = torch.max(S, dim=1).values
                new_m = torch.maximum(m, block_row_max)
                l *= torch.exp(m - new_m)

                exp_scores = torch.exp(S - new_m.unsqueeze(1))
                l += torch.sum(exp_scores, dim=1)

                probs_block = exp_scores / l.unsqueeze(1)
                col_sum[j0:j1] += probs_block.sum(dim=0)
                m = new_m

            out[b] += col_sum / H
            
    return out


def _get(module, names):
    for n in names:
        if hasattr(module, n):
            return getattr(module, n)
    return None


def _get_heads_from_model(model, last_attn_module):
    H_q = getattr(getattr(model, "config", object()), "num_attention_heads", None)
    H_kv = getattr(getattr(model, "config", object()), "num_key_value_heads", None)
    return int(H_q), int(H_kv)


def _attach_qk_hooks(last_attn_module, model):
    """
    ä» config è¯» H_q/H_kvï¼›Q æŒ‰ H_q reshapeï¼ŒK æŒ‰ H_kv reshapeï¼Œå†æŠŠ K æ‰©å±•åˆ° H_qã€‚
    """
    q_linear = _get(last_attn_module, ["q_proj", "wq", "query", "q"])
    k_linear = _get(last_attn_module, ["k_proj", "wk", "key", "k"])
    if q_linear is None or k_linear is None:
        return []

    H_q, H_kv = _get_heads_from_model(model, last_attn_module)
    group = H_q // H_kv
    captured["__H_q__"] = torch.tensor(H_q)
    captured["__H_kv__"] = torch.tensor(H_kv)

    hooks = []

    def _grab_q(module, inp, out):
        B, L, Dall_q = out.shape
        assert Dall_q % H_q == 0
        D = Dall_q // H_q
        q = out.view(B, L, H_q, D).permute(0, 2, 1, 3).contiguous()
        captured["q_linear"] = q.detach()

    def _grab_k(module, inp, out):
        B, L, Dall_k = out.shape
        assert Dall_k % H_kv == 0
        D = Dall_k // H_kv
        k = out.view(B, L, H_kv, D).permute(0, 2, 1, 3).contiguous()
        if H_kv != H_q:
            k = k.repeat_interleave(group, dim=1)
        captured["k_linear"] = k.detach()

    hooks.append(q_linear.register_forward_hook(_grab_q))
    hooks.append(k_linear.register_forward_hook(_grab_k))
    return hooks


def _apply_rope_if_possible(q, k, last_attn_module, seq_len):
    """å°è¯•åº”ç”¨RoPE"""
    try:
        rotary = getattr(last_attn_module, "rotary_emb", None) or getattr(last_attn_module, "rope", None)
        if rotary is None:
            return q, k

        B, H, L, D = q.shape
        if hasattr(rotary, "forward"):
            cos, sin = rotary(torch.empty(B*H, L, D, device=q.device, dtype=q.dtype), seq_len=L)
        elif hasattr(rotary, "get_cos_sin"):
            cos, sin = rotary.get_cos_sin(L, device=q.device, dtype=q.dtype)
        else:
            return q, k

        if cos.dim() == 2:
            cos = cos.unsqueeze(0).unsqueeze(0).expand(B, H, L, D)
            sin = sin.unsqueeze(0).unsqueeze(0).expand(B, H, L, D)
        elif cos.dim() == 3:
            cos = cos.unsqueeze(0).expand(B, -1, -1, -1)
            sin = sin.unsqueeze(0).expand(B, -1, -1, -1)

        def _rope(a, cos, sin):
            D2 = a.shape[-1] // 2
            a1, a2 = a[..., :D2], a[..., D2:]
            rot = torch.cat([-a2, a1], dim=-1)
            return a * cos.to(a.dtype) + rot * sin.to(a.dtype)

        return _rope(q, cos, sin), _rope(k, cos, sin)
    except Exception:
        return q, k


def calc_attentions_streaming(seq: str, model, tokenizer, device, block_cols=1024, causal=False) -> List[float]:
    """
    ä½¿ç”¨æµå¼æ–¹æ³•è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    
    å‚æ•°:
        causal: True=å› æœmaskï¼ˆåªçœ‹å‰é¢ï¼‰ï¼ŒFalse=åŒå‘æ³¨æ„åŠ›ï¼ˆçœ‹å…¨éƒ¨ï¼‰
    """
    captured.clear()

    tokenizer.model_max_length = int(1e9)
    inputs = tokenizer(seq, return_tensors="pt", truncation=False)
    if 'token_type_ids' in inputs:
        del inputs['token_type_ids']
    inputs = {k: v.to(device) for k, v in inputs.items()}

    last_attn = model.model.layers[-1].self_attn
    if last_attn is None:
        raise ValueError("æœªæ‰¾åˆ°æœ€åä¸€å±‚ self-attention æ¨¡å—")

    hooks = _attach_qk_hooks(last_attn, model)
    try:
        model.eval()
        with torch.no_grad():
            _ = model(**inputs)
    finally:
        for h in hooks:
            try: h.remove()
            except Exception: pass

    q = captured.get("q_linear", None)
    k = captured.get("k_linear", None)
    if q is None or k is None:
        raise ValueError("æœªæ•è·åˆ° Q/K")

    q, k = q.to(device), k.to(device)
    q, k = _apply_rope_if_possible(q, k, last_attn, q.shape[2])

    # ä½¿ç”¨ä¼ å…¥çš„causalå‚æ•°
    col_sums = attn_column_sums_streaming(q, k, causal=causal, block_cols=block_cols)
    vec = col_sums[0]
    return vec.detach().cpu().float().numpy().tolist()


# ==================== ä¸»ç¨‹åºï¼ˆæ‰¹é‡å¤„ç†JSONï¼‰====================

def parse_arguments():
    parser = argparse.ArgumentParser(description="æµå¼æ³¨æ„åŠ›åˆ†æ•°æå–ï¼ˆå†…å­˜é«˜æ•ˆç‰ˆï¼‰")
    
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_dir", type=str, required=True, help="è¾“å…¥JSONç›®å½•")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºJSONç›®å½•")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"])
    parser.add_argument("--file_pattern", type=str, default="*.json", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œå¦‚ block_*.json")
    parser.add_argument("--output_suffix", type=str, default="_streaming", help="è¾“å‡ºæ–‡ä»¶åç¼€")
    parser.add_argument("--block_cols", type=int, default=1024, help="æµå¼è®¡ç®—çš„å—å¤§å°ï¼ˆè¶Šå°è¶Šçœå†…å­˜ï¼‰")
    parser.add_argument("--keep_original_sequence", action="store_true", help="ä¿ç•™åŸå§‹åºåˆ—å­—æ®µ")
    parser.add_argument("--original_seq_field", type=str, default="original_sequence", help="åŸå§‹åºåˆ—å­—æ®µå")
    parser.add_argument("--causal", action="store_true", 
                       help="ä½¿ç”¨å› æœmaskï¼ˆåªçœ‹å‰é¢çš„tokenï¼‰ï¼Œé»˜è®¤ä¸ºåŒå‘æ³¨æ„åŠ›")
    
    return parser.parse_args()


def main():
    args = parse_arguments()
    
    print("=" * 80)
    print("æµå¼æ³¨æ„åŠ›åˆ†æ•°æå–ï¼ˆå†…å­˜é«˜æ•ˆç‰ˆï¼‰")
    print("=" * 80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å—å¤§å°: {args.block_cols}")
    print("=" * 80)
    
    os.makedirs(args.output_dir, exist_ok=True)
    device = torch.device(args.device)
    
    # åŠ è½½æ¨¡å‹
    print("\nåŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    ).to(device)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # è·å–JSONæ–‡ä»¶
    json_files = list(Path(args.input_dir).glob(args.file_pattern))
    print(f"\næ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
    
    total_samples = 0
    
    for json_file in json_files:
        output_file = Path(args.output_dir) / f"{json_file.stem}{args.output_suffix}.json"
        
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ–‡ä»¶: {json_file.name}")
        
        dataset = load_dataset("json", data_files=str(json_file), split="all")
        samples = list(dataset)
        
        print(f"æ ·æœ¬æ•°: {len(samples)}")
        
        processed = []
        
        for sample in tqdm(samples, desc="å¤„ç†æ ·æœ¬"):
            seq = sample["sequence"]
            
            try:
                # ğŸ”¥ ä½¿ç”¨å‚æ•°æ§åˆ¶æ˜¯å¦causal
                attn_scores = calc_attentions_streaming(
                    seq, model, tokenizer, device, 
                    block_cols=args.block_cols,
                    causal=args.causal  # â† æ–°å¢å‚æ•°
                )
                
                new_sample = sample.copy()
                if args.keep_original_sequence:
                    new_sample[args.original_seq_field] = seq
                new_sample["sequence"] = attn_scores
                
                processed.append(new_sample)
                
            except Exception as e:
                print(f"\nâš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
                processed.append(sample)
        
        # ä¿å­˜
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed, f, indent=2, ensure_ascii=False)
        
        total_samples += len(processed)
        print(f"âœ… ä¿å­˜åˆ°: {output_file.name}")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å¤„ç†å®Œæˆ!")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print("=" * 80)


if __name__ == "__main__":
    main()
