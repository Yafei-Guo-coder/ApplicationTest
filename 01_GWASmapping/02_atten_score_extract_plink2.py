#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä»PLINK2æ ¼å¼(pgen/psam/pvar)ç”Ÿæˆæ ·æœ¬ä¸€è‡´æ€§åºåˆ—
æ”¯æŒSNPã€Indel(æ’å…¥/ç¼ºå¤±)ã€å¤šç­‰ä½å˜å¼‚
"""

import os
import json
import numpy as np
import pandas as pd
import pysam
from pgenlib import PgenReader
from tqdm import tqdm

# =====================
# è·¯å¾„é…ç½®
# =====================
bed_file = "GAD1.bed"
pheno_file = "/mnt/zzb/default/Workspace/Rice-Genome/application/GWAS_fine_mapping/RiceGWAScohort/phenotyping_data/2_3K_Rice_pheno"
# pgen_prefix = "/mnt/zzb/default/Workspace/guoyafei/riceData/rice4k_geno_add_del"  # .pgen/.psam/.pvarçš„å…±åŒå‰ç¼€
pgen_prefix = "RICE_RP_GAD1"
fasta_file = "/mnt/zzb/default/Public/OsGenos/Oryza_sativa/chromosome/GCA_001433935.1_IRGSP-1.0_genomic.fna.gz"

target_pheno_col = "awn_presence"
out_dir = "json_blocks_APTrue_with_indel"
os.makedirs(out_dir, exist_ok=True)

# =====================
# æŸ“è‰²ä½“æ˜ å°„
# =====================
chrom_map = {
    '1': 'AP014957.1', '2': 'AP014958.1', '3': 'AP014959.1',
    '4': 'AP014960.1', '5': 'AP014961.1', '6': 'AP014962.1',
    '7': 'AP014963.1', '8': 'AP014964.1', '9': 'AP014965.1',
    '10': 'AP014966.1', '11': 'AP014967.1', '12': 'AP014968.1'
}

# =====================
# 1. è¯»å–è¡¨å‹
# =====================
print("â¡ï¸ è¯»å–è¡¨å‹æ•°æ® ...")
pheno = pd.read_csv(pheno_file, sep="\t")
pheno = pheno.dropna(subset=[target_pheno_col])
pheno = pheno.set_index("SampleID")
print(f"âœ… æœ‰æ•ˆè¡¨å‹æ ·æœ¬æ•°: {len(pheno)}")

# =====================
# 2. è¯»å–BEDåŒºé—´
# =====================
print("â¡ï¸ è¯»å–BEDæ–‡ä»¶ ...")
bed_df = pd.read_csv(bed_file, sep="\t", header=None, names=["chrom", "start", "end"])
print(f"âœ… BED åŒºé—´æ•°: {len(bed_df)}")

# =====================
# 3. è¯»å–FASTAå‚è€ƒåºåˆ—
# =====================
print("â¡ï¸ æ‰“å¼€FASTAæ–‡ä»¶ ...")
fasta = pysam.FastaFile(fasta_file)

# =====================
# 4. è¯»å–PLINK2æ–‡ä»¶
# =====================
print("â¡ï¸ è¯»å–PLINK2æ–‡ä»¶ ...")

# 4.1 è¯»å–.psam (æ ·æœ¬ä¿¡æ¯)
psam_file = f"{pgen_prefix}.psam"
print(f"  è¯»å–: {psam_file}")

# å…ˆæ£€æŸ¥æ–‡ä»¶æ ¼å¼
with open(psam_file, 'r') as f:
    first_line = f.readline().strip()
    print(f"  PSAMé¦–è¡Œ: {first_line}")

# è¯»å–PSAMï¼Œä¿ç•™#å¼€å¤´çš„åˆ—å
psam = pd.read_csv(psam_file, sep="\t")
print(f"  åŸå§‹åˆ—å: {psam.columns.tolist()}")

# çµæ´»å¤„ç†åˆ—åï¼ˆå¯èƒ½æ˜¯IID, #IID, æˆ–FID IIDæ ¼å¼ï¼‰
possible_id_cols = ['IID', '#IID', 'iid', '#iid']
sample_col = None

for col in possible_id_cols:
    if col in psam.columns:
        sample_col = col
        break

# å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ç¬¬ä¸€åˆ—æˆ–ç¬¬äºŒåˆ—
if sample_col is None:
    if len(psam.columns) >= 2:
        # PLINK2æ ¼å¼é€šå¸¸æ˜¯: FID IID æˆ– #FID IID
        # ä½¿ç”¨ç¬¬äºŒåˆ—ä½œä¸ºæ ·æœ¬ID
        sample_col = psam.columns[1]
        print(f"  âš ï¸  æœªæ‰¾åˆ°æ ‡å‡†IIDåˆ—ï¼Œä½¿ç”¨ç¬¬2åˆ—: {sample_col}")
    else:
        # ä½¿ç”¨ç¬¬ä¸€åˆ—
        sample_col = psam.columns[0]
        print(f"  âš ï¸  æœªæ‰¾åˆ°æ ‡å‡†IIDåˆ—ï¼Œä½¿ç”¨ç¬¬1åˆ—: {sample_col}")

sample_ids = psam[sample_col].tolist()
print(f"âœ… PSAMæ ·æœ¬æ•°: {len(sample_ids)}")
print(f"  ä½¿ç”¨åˆ—: {sample_col}")
print(f"  ç¤ºä¾‹æ ·æœ¬: {sample_ids[:5]}")

# 4.2 è¯»å–.pvar (å˜å¼‚ä¿¡æ¯)
pvar_file = f"{pgen_prefix}.pvar"
print(f"  è¯»å–: {pvar_file}")

# è®¡ç®—éœ€è¦è·³è¿‡çš„å…ƒæ•°æ®è¡Œæ•°ï¼ˆ##å¼€å¤´çš„è¡Œï¼‰
skip_rows = 0
with open(pvar_file, 'r') as f:
    for line in f:
        if line.startswith('##'):
            skip_rows += 1
        else:
            print(f"  PVARæ•°æ®é¦–è¡Œ: {line.strip()}")
            break

print(f"  è·³è¿‡å…ƒæ•°æ®è¡Œ: {skip_rows}")

# è¯»å–PVARï¼Œè·³è¿‡##å¼€å¤´çš„è¡Œ
pvar = pd.read_csv(pvar_file, sep="\t", skiprows=skip_rows)
print(f"  åŸå§‹åˆ—å: {pvar.columns.tolist()}")

# æ ‡å‡†åŒ–åˆ—åï¼ˆç§»é™¤#ï¼‰
pvar.columns = pvar.columns.str.replace('#', '')
print(f"  æ ‡å‡†åŒ–ååˆ—å: {pvar.columns.tolist()}")

# æ£€æŸ¥å¿…éœ€åˆ—
required_cols = ['CHROM', 'POS', 'REF', 'ALT']
missing_cols = [col for col in required_cols if col not in pvar.columns]
if missing_cols:
    raise ValueError(f"PVARæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}\nå¯ç”¨åˆ—: {pvar.columns.tolist()}")

print(f"âœ… PVARå˜å¼‚æ•°: {len(pvar)}")

# æ˜¾ç¤ºå‰å‡ è¡Œ
print("\n  PVARå‰3è¡Œ:")
display_cols = [c for c in ['CHROM', 'POS', 'ID', 'REF', 'ALT'] if c in pvar.columns]
print(pvar.head(3)[display_cols].to_string())

# 4.3 æ‰“å¼€.pgen (åŸºå› å‹æ•°æ®)
pgen_file = f"{pgen_prefix}.pgen"
pgen_reader = PgenReader(bytes(pgen_file, 'utf8'))
n_variants = pgen_reader.get_variant_ct()
n_samples = pgen_reader.get_raw_sample_ct()
print(f"âœ… PGEN: {n_variants} å˜å¼‚, {n_samples} æ ·æœ¬")

# =====================
# 5. æ ·æœ¬è¿‡æ»¤å’Œæ˜ å°„
# =====================
# æ‰¾åˆ°æ—¢åœ¨è¡¨å‹ä¸­åˆåœ¨åŸºå› å‹ä¸­çš„æ ·æœ¬
pheno_samples = [s for s in pheno.index if s in sample_ids]
sample_to_idx = {sid: i for i, sid in enumerate(sample_ids)}
pheno_idx = np.array([sample_to_idx[s] for s in pheno_samples])
labels = pheno.loc[pheno_samples, target_pheno_col].values

print(f"âœ… å…±åŒæ ·æœ¬æ•°: {len(pheno_samples)}")
print(f"ç¤ºä¾‹æ ·æœ¬: {pheno_samples[:5]}")

# =====================
# 6. è¾…åŠ©å‡½æ•°
# =====================
def classify_variant(ref, alt):
    """
    åˆ¤æ–­å˜å¼‚ç±»å‹
    è¿”å›: 'SNP', 'INS' (insertion), 'DEL' (deletion), 'COMPLEX'
    """
    ref = str(ref).upper()
    alt = str(alt).upper()

    len_ref = len(ref)
    len_alt = len(alt)

    if len_ref == 1 and len_alt == 1:
        return 'SNP'
    elif len_ref < len_alt:
        return 'INS'  # æ’å…¥
    elif len_ref > len_alt:
        return 'DEL'  # ç¼ºå¤±
    else:
        return 'COMPLEX'


def apply_variant_to_sequence(ref_seq, variant_list):
    """
    å°†å˜å¼‚åº”ç”¨åˆ°å‚è€ƒåºåˆ—

    å‚æ•°:
        ref_seq: å‚è€ƒåºåˆ—(å­—ç¬¦ä¸²)
        variant_list: [(offset, variant_type, ref, alt), ...]
                     offset: ç›¸å¯¹äºref_seqèµ·å§‹çš„ä½ç½®(0-based)

    è¿”å›:
        ä¿®æ”¹åçš„åºåˆ—(å­—ç¬¦ä¸²)
    """
    # æŒ‰ä½ç½®ä»åå¾€å‰æ’åº,é¿å…æ’å…¥/åˆ é™¤å½±å“åç»­ä½ç½®
    variant_list = sorted(variant_list, key=lambda x: x[0], reverse=True)

    seq = list(ref_seq)

    for offset, var_type, ref, alt in variant_list:
        if offset < 0 or offset >= len(seq):
            continue

        if var_type == 'SNP':
            # ç®€å•æ›¿æ¢
            seq[offset] = alt

        elif var_type == 'INS':
            # æ’å…¥: åœ¨offsetä½ç½®åæ’å…¥é¢å¤–ç¢±åŸº
            # ä¾‹: REF=A, ALT=ATT, æ’å…¥TT
            # offsetä½ç½®çš„ç¢±åŸºæ›¿æ¢ä¸ºalt[0], ç„¶åæ’å…¥alt[1:]
            seq[offset] = alt[0]
            insert_bases = alt[1:]
            for i, base in enumerate(insert_bases):
                seq.insert(offset + 1 + i, base)

        elif var_type == 'DEL':
            # ç¼ºå¤±: æ›¿æ¢ä¸ºN
            # ä¾‹: REF=ATT, ALT=A, ç¼ºå¤±2ä¸ªç¢±åŸº
            del_len = len(ref) - len(alt)
            # ä¿ç•™alt[0]åœ¨offsetä½ç½®, åç»­del_lenä¸ªä½ç½®ç”¨Næ›¿æ¢
            seq[offset] = alt[0] if len(alt) > 0 else 'N'
            for i in range(1, len(ref)):
                if offset + i < len(seq):
                    seq[offset + i] = 'N'

        elif var_type == 'COMPLEX':
            # å¤æ‚å˜å¼‚: ç®€å•å¤„ç†ä¸ºæ›¿æ¢+å¡«å……N
            seq[offset] = alt[0] if len(alt) > 0 else 'N'
            if len(ref) > 1:
                for i in range(1, len(ref)):
                    if offset + i < len(seq):
                        seq[offset + i] = 'N'

    return ''.join(seq)


# =====================
# 7. ä¸»å¾ªç¯: é€blockå¤„ç†
# =====================
print("\n" + "="*60)
print("å¼€å§‹å¤„ç†blocks...")
print("="*60)

for block_id, row in tqdm(bed_df.iterrows(), total=len(bed_df), desc="Processing blocks"):
    chrom = str(row['chrom'])
    start = int(row['start'])
    end = int(row['end'])
    block_name = f"block_{block_id+1}"

    # è·³è¿‡ä¸åœ¨æ˜ å°„è¡¨ä¸­çš„æŸ“è‰²ä½“
    if chrom not in chrom_map:
        print(f"âš ï¸  è·³è¿‡ {block_name}: æŸ“è‰²ä½“ {chrom} ä¸åœ¨æ˜ å°„è¡¨ä¸­")
        continue

    # æå–å‚è€ƒåºåˆ—
    try:
        ref_seq = fasta.fetch(chrom_map[chrom], start, end).upper()
    except Exception as e:
        print(f"âš ï¸  è·³è¿‡ {block_name}: FASTAæå–å¤±è´¥ - {e}")
        continue

    # ç­›é€‰è¯¥blockå†…çš„å˜å¼‚
    mask = (
        (pvar['CHROM'].astype(str) == chrom) &
        (pvar['POS'] >= start + 1) &
        (pvar['POS'] <= end)
    )
    var_idx = np.where(mask)[0]

    if len(var_idx) == 0:
        print(f"âš ï¸  è·³è¿‡ {block_name}: æ— å˜å¼‚")
        continue

    # è·å–å˜å¼‚ä¿¡æ¯
    variants_in_block = pvar.iloc[var_idx].copy()
    var_positions = variants_in_block['POS'].values
    var_refs = variants_in_block['REF'].values
    var_alts = variants_in_block['ALT'].values

    print(f"\nğŸ“ {block_name}: {len(var_idx)} å˜å¼‚")

    # é€æ ·æœ¬ç”Ÿæˆåºåˆ—
    json_list = []

    for i, sample in enumerate(tqdm(pheno_samples, desc=f"  {block_name} æ ·æœ¬", leave=False)):
        sample_idx = pheno_idx[i]

        # è¯¥æ ·æœ¬åœ¨æ­¤blockçš„æ‰€æœ‰å˜å¼‚åˆ—è¡¨
        sample_variants = []

        for j, global_var_idx in enumerate(var_idx):
            # è¯»å–è¯¥å˜å¼‚çš„åŸºå› å‹
            # pgenlibè¿”å›çš„æ˜¯dosageæˆ–ç¡¬ç¼–ç åŸºå› å‹
            # æˆ‘ä»¬éœ€è¦alleleç¼–ç : 0=REF/REF, 1=REF/ALT, 2=ALT/ALT, -9=missing

            geno_array = np.empty(n_samples, dtype=np.int32)
            pgen_reader.read(global_var_idx, geno_array)
            geno = geno_array[sample_idx]

            # è·å–è¯¥å˜å¼‚çš„REFå’ŒALT
            ref = str(var_refs[j]).upper()
            alt_str = str(var_alts[j]).upper()

            # å¤„ç†å¤šç­‰ä½åŸºå›  (ALTå¯èƒ½æ˜¯ "A,T,G" è¿™ç§æ ¼å¼)
            alts = alt_str.split(',')

            # æ ¹æ®åŸºå› å‹é€‰æ‹©ç­‰ä½åŸºå› 
            if geno == -9 or geno < 0:
                # missing genotype: ç”¨Nå¡«å……
                applied_allele = 'N' * len(ref)
            elif geno == 0:
                # REF/REF: ä¸ä¿®æ”¹(å‚è€ƒåºåˆ—å·²ç»æ˜¯REF)
                continue
            elif geno == 1:
                # REF/ALT: ä½¿ç”¨ç¬¬ä¸€ä¸ªALT (æ‚åˆä¸€èˆ¬æ˜¾ç¤ºALT)
                applied_allele = alts[0] if len(alts) > 0 else ref
            elif geno == 2:
                # ALT/ALT: ä½¿ç”¨ç¬¬ä¸€ä¸ªALT
                applied_allele = alts[0] if len(alts) > 0 else ref
            else:
                # å…¶ä»–ç¼–ç (å¦‚å¤šç­‰ä½çš„å¤æ‚ç¼–ç ),ç®€åŒ–å¤„ç†
                allele_idx = min(geno - 1, len(alts) - 1)
                applied_allele = alts[allele_idx] if allele_idx >= 0 else ref

            # è®¡ç®—ç›¸å¯¹å‚è€ƒåºåˆ—çš„offset (0-based)
            offset = var_positions[j] - start - 1

            # åˆ¤æ–­å˜å¼‚ç±»å‹
            var_type = classify_variant(ref, applied_allele)

            sample_variants.append((offset, var_type, ref, applied_allele))

        # åº”ç”¨æ‰€æœ‰å˜å¼‚åˆ°å‚è€ƒåºåˆ—
        consensus_seq = apply_variant_to_sequence(ref_seq, sample_variants)

        json_list.append({
            "label": int(labels[i]),
            "spec": sample,
            "loc": block_name,
            "sequence": consensus_seq
        })

    # è¾“å‡ºJSON
    out_path = os.path.join(out_dir, f"{block_name}.json")
    with open(out_path, "w") as f:
        json.dump(json_list, f, indent=2)

    print(f"âœ… {block_name} å®Œæˆ | å˜å¼‚æ•°={len(var_idx)} | æ ·æœ¬æ•°={len(pheno_samples)}")

# =====================
# 8. æ¸…ç†
# =====================
fasta.close()
pgen_reader.close()

print("\n" + "="*60)
print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
print(f"è¾“å‡ºç›®å½•: {out_dir}")
print("="*60)
