#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ ‡å‡†åŒ–attentionåˆ†æ•°é•¿åº¦
å°†å› indelå¯¼è‡´çš„ä¸åŒé•¿åº¦åºåˆ—çš„åˆ†æ•°æ ‡å‡†åŒ–ä¸ºå‚è€ƒåºåˆ—é•¿åº¦
- æ’å…¥(INS): å¤šä¸ªç¢±åŸºçš„åˆ†æ•°å¹³å‡ä¸º1ä¸ª
- ç¼ºå¤±(DEL): ä¿æŒNä½ç½®çš„åˆ†æ•°
- æ”¯æŒå¤šç­‰ä½åŸºå› ï¼ˆALTå¯èƒ½æ˜¯ "T,DEL" æˆ– "A,T,DEL"ï¼‰
"""

import os
import json
import numpy as np
import pandas as pd
import pysam
from cyvcf2 import VCF
from pathlib import Path
from tqdm import tqdm
import argparse

def parse_args():
    parser = argparse.ArgumentParser(description='æ ‡å‡†åŒ–attentionåˆ†æ•°é•¿åº¦')
    parser.add_argument('--json_dir', type=str, required=True,
                       help='åŒ…å«åŸå§‹JSONæ–‡ä»¶çš„ç›®å½• (block_*.json)')
    parser.add_argument('--seq_json_dir', type=str, required=True,
                       help='åŒ…å«åºåˆ—JSONçš„ç›®å½• (ç”¨äºè·å–å®é™…åºåˆ—)')
    parser.add_argument('--bed_file', type=str, required=True,
                       help='BEDæ–‡ä»¶,å®šä¹‰å‚è€ƒåŒºé—´')
    parser.add_argument('--vcf_file', type=str, required=True,
                       help='VCFæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ.vcf, .vcf.gz, .bcfï¼‰')
    parser.add_argument('--fasta_file', type=str, required=True,
                       help='å‚è€ƒåŸºå› ç»„FASTA')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    return parser.parse_args()


# æŸ“è‰²ä½“æ˜ å°„
chrom_map = {
    '1': 'AP014957.1', '2': 'AP014958.1', '3': 'AP014959.1',
    '4': 'AP014960.1', '5': 'AP014961.1', '6': 'AP014962.1',
    '7': 'AP014963.1', '8': 'AP014964.1', '9': 'AP014965.1',
    '10': 'AP014966.1', '11': 'AP014967.1', '12': 'AP014968.1'
}


def classify_variant(ref, alt):
    """
    åˆ¤æ–­å•ä¸ªç­‰ä½åŸºå› çš„å˜å¼‚ç±»å‹
    
    æ³¨æ„ï¼šè¿™é‡Œçš„altæ˜¯å•ä¸ªç­‰ä½åŸºå› ï¼Œä¸æ˜¯"T,DEL"è¿™ç§å¤šç­‰ä½æ ¼å¼
    """
    # å¤„ç†DELæ ‡è®°
    if alt in ['DEL', '<DEL>', '*', '.']:
        return 'DEL'
    
    ref = str(ref).upper()
    alt = str(alt).upper()
    
    if len(ref) == 1 and len(alt) == 1:
        return 'SNP'
    elif len(ref) < len(alt):
        return 'INS'
    elif len(ref) > len(alt):
        return 'DEL'
    else:
        return 'COMPLEX'


def get_variant_info(ref, alt):
    """
    è·å–å˜å¼‚ä¿¡æ¯
    
    è¿”å›: (variant_type, ins_length)
    """
    var_type = classify_variant(ref, alt)
    
    if alt in ['DEL', '<DEL>', '*', '.']:
        ins_length = 0
    else:
        ins_length = max(0, len(alt) - len(ref))
    
    return var_type, ins_length


def load_variants_in_region_from_vcf(vcf_file, chrom, start, end):
    """
    ä»VCFæ–‡ä»¶è¯»å–æŒ‡å®šåŒºé—´çš„å˜å¼‚ä¿¡æ¯
    
    è¿”å›: DataFrame with columns [POS, REF, ALT, variant_type, ins_length]
    æ³¨æ„ï¼šè¿™é‡Œåªè®°å½•æ¯ä¸ªä½ç½®å¯èƒ½å‡ºç°çš„æœ€å¤§æ’å…¥é•¿åº¦
    """
    vcf = VCF(vcf_file)
    
    variants_list = []
    
    # VCFæŸ“è‰²ä½“åå¯èƒ½æ˜¯"4"æˆ–"AP014960.1"
    vcf_chrom_names = [str(chrom), chrom_map.get(str(chrom), str(chrom))]
    
    for vcf_chrom in vcf_chrom_names:
        try:
            for variant in vcf(f"{vcf_chrom}:{start}-{end}"):
                pos = variant.POS
                ref = variant.REF
                alts = variant.ALT
                
                # åˆ†ææ‰€æœ‰ALTç­‰ä½åŸºå› ï¼Œæ‰¾å‡ºæœ€æç«¯çš„æƒ…å†µ
                max_ins_len = 0
                variant_types = set()
                
                for alt in alts:
                    var_type, ins_len = get_variant_info(ref, alt)
                    variant_types.add(var_type)
                    max_ins_len = max(max_ins_len, ins_len)
                
                # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªALTæ˜¯INSï¼Œå°±æ ‡è®°ä¸ºINS
                if 'INS' in variant_types:
                    final_type = 'INS'
                elif 'DEL' in variant_types:
                    final_type = 'DEL'
                elif 'SNP' in variant_types:
                    final_type = 'SNP'
                else:
                    final_type = 'COMPLEX'
                
                variants_list.append({
                    'POS': pos,
                    'REF': ref,
                    'ALT': ','.join(alts),
                    'variant_type': final_type,
                    'ins_length': max_ins_len
                })
            
            if len(variants_list) > 0:
                break
        except Exception as e:
            continue
    
    vcf.close()
    
    if len(variants_list) == 0:
        return pd.DataFrame(columns=['POS', 'REF', 'ALT', 'variant_type', 'ins_length'])
    
    return pd.DataFrame(variants_list).sort_values('POS')


def get_sample_variants_from_vcf(vcf_file, chrom, start, end, sample_id, all_samples):
    """
    ä»VCFè·å–ç‰¹å®šæ ·æœ¬å®é™…æºå¸¦çš„å˜å¼‚
    
    è¿”å›: list of (position, variant_type, ins_length)
    """
    vcf = VCF(vcf_file)
    
    # æ‰¾åˆ°æ ·æœ¬ç´¢å¼•
    if sample_id not in all_samples:
        vcf.close()
        return []
    
    sample_idx = all_samples.index(sample_id)
    
    sample_variants = []
    
    # VCFæŸ“è‰²ä½“åå¯èƒ½æ˜¯"4"æˆ–"AP014960.1"
    vcf_chrom_names = [str(chrom), chrom_map.get(str(chrom), str(chrom))]
    
    for vcf_chrom in vcf_chrom_names:
        try:
            for variant in vcf(f"{vcf_chrom}:{start}-{end}"):
                # è·å–è¯¥æ ·æœ¬çš„åŸºå› å‹
                gt = variant.genotypes[sample_idx]
                allele1, allele2 = gt[0], gt[1]
                
                # è·³è¿‡å‚è€ƒå‹ 0/0
                if allele1 == 0 and allele2 == 0:
                    continue
                
                # è·³è¿‡ç¼ºå¤±åŸºå› å‹
                if allele1 == -1 or allele2 == -1:
                    continue
                # ğŸ”¥ æ­£ç¡®é€»è¾‘ï¼šé€ä¸ªç­‰ä½åŸºå› åˆ¤æ–­ï¼Œåªè®°å½• INS
                alleles = [allele1, allele2]

                for allele_idx in alleles:
                    if allele_idx <= 0:
                        continue

                    if allele_idx - 1 >= len(variant.ALT):
                        continue

                    alt = variant.ALT[allele_idx - 1]
                    ref = variant.REF

                    var_type, ins_len = get_variant_info(ref, alt)

                    # åªæœ‰å½“è¯¥æ ·æœ¬çœŸçš„æºå¸¦ INS æ—¶æ‰è®°å½•
                    if var_type == 'INS' and ins_len > 0:
                        sample_variants.append({
                            'POS': variant.POS,
                            'ins_length': ins_len
                        })

            
            if len(sample_variants) > 0 or vcf_chrom == vcf_chrom_names[-1]:
                break
        except Exception as e:
            continue
    
    vcf.close()
    return sample_variants


def build_position_mapping_with_sample_variants(ref_seq, sample_variants, ref_start):
    """
    ä½¿ç”¨æ ·æœ¬å®é™…æºå¸¦çš„å˜å¼‚æ„å»ºæ˜ å°„
    
    å‚æ•°:
        ref_seq: å‚è€ƒåºåˆ—
        sample_variants: è¯¥æ ·æœ¬å®é™…æºå¸¦çš„å˜å¼‚åˆ—è¡¨
        ref_start: å‚è€ƒåºåˆ—èµ·å§‹ä½ç½®
    
    è¿”å›:
        mapping: æ ·æœ¬åºåˆ—ä½ç½® -> å‚è€ƒåºåˆ—ä½ç½®
    """
    ref_len = len(ref_seq)
    
    # å¦‚æœæ²¡æœ‰æ’å…¥å˜å¼‚ï¼Œé•¿åº¦ç›¸åŒ
    if not sample_variants:
        return list(range(ref_len))
    
    # æ„å»ºå˜å¼‚å­—å…¸ï¼šref_offset -> ins_length
    var_dict = {}
    for var in sample_variants:
        ref_offset = var['POS'] - ref_start - 1
        if 0 <= ref_offset < ref_len:
            var_dict[ref_offset] = var['ins_length']
    
    # æ„å»ºæ˜ å°„
    mapping = []
    ref_pos = 0
    
    while ref_pos < ref_len:
        # å½“å‰å‚è€ƒä½ç½®
        mapping.append(ref_pos)
        
        # æ£€æŸ¥è¯¥ä½ç½®æ˜¯å¦æœ‰æ’å…¥
        if ref_pos in var_dict:
            ins_len = var_dict[ref_pos]
            # æ·»åŠ æ’å…¥çš„ç¢±åŸºï¼ˆæ ‡è®°ä¸º-1ï¼‰
            for _ in range(ins_len):
                mapping.append(-1)
        
        ref_pos += 1
    
    return mapping


def normalize_attention_scores(scores, mapping, ref_len):
    """
    æ ¹æ®æ˜ å°„å…³ç³»æ ‡å‡†åŒ–attentionåˆ†æ•°
    
    å‚æ•°:
        scores: åŸå§‹åˆ†æ•°åˆ—è¡¨ (é•¿åº¦å¯èƒ½ä¸ç­‰äºref_len)
        mapping: ä½ç½®æ˜ å°„ (ç”±build_position_mappingç”Ÿæˆ)
        ref_len: å‚è€ƒåºåˆ—é•¿åº¦
    
    è¿”å›:
        normalized_scores: æ ‡å‡†åŒ–åçš„åˆ†æ•°åˆ—è¡¨ (é•¿åº¦=ref_len)
    """
    if len(scores) == ref_len and len(mapping) == ref_len:
        # é•¿åº¦å·²ç»ä¸€è‡´,ç›´æ¥è¿”å›
        return scores
    
    # åˆå§‹åŒ–è¾“å‡º
    normalized = [0.0] * ref_len
    counts = [0] * ref_len  # è®°å½•æ¯ä¸ªä½ç½®ç´¯ç§¯äº†å¤šå°‘ä¸ªåˆ†æ•°
    
    # éå†æ ·æœ¬åºåˆ—çš„æ¯ä¸ªä½ç½®
    for sample_pos, score in enumerate(scores):
        if sample_pos >= len(mapping):
            break
        
        ref_pos = mapping[sample_pos]
        
        if ref_pos == -1:
            # è¿™æ˜¯æ’å…¥çš„ç¢±åŸº,éœ€è¦å¹³å‡åˆ°å‰ä¸€ä¸ªä½ç½®
            # æ‰¾åˆ°å‰ä¸€ä¸ªéæ’å…¥ä½ç½®
            prev_pos = sample_pos - 1
            while prev_pos >= 0 and mapping[prev_pos] == -1:
                prev_pos -= 1
            
            if prev_pos >= 0:
                target_ref_pos = mapping[prev_pos]
                if 0 <= target_ref_pos < ref_len:
                    normalized[target_ref_pos] += score
                    counts[target_ref_pos] += 1
        else:
            # æ­£å¸¸ä½ç½®
            if 0 <= ref_pos < ref_len:
                normalized[ref_pos] += float(score)
                counts[ref_pos] += 1
    
    # è®¡ç®—å¹³å‡å€¼
    for i in range(ref_len):
        if counts[i] > 0:
            normalized[i] /= counts[i]
        # å¦‚æœcounts[i]=0,ä¿æŒ0.0
    
    return normalized


def process_block(block_name, json_file, seq_json_file, bed_row, 
                  vcf_file, fasta, output_dir, all_samples):
    """
    å¤„ç†å•ä¸ªblock
    """
    chrom = str(bed_row['chrom'])
    start = int(bed_row['start'])
    end = int(bed_row['end'])
    
    print(f"\nğŸ“ {block_name}: chr{chrom}:{start}-{end}")
    
    # 1. è¯»å–å‚è€ƒåºåˆ—
    try:
        ref_seq = fasta.fetch(chrom_map[chrom], start, end).upper()
        ref_len = len(ref_seq)
    except Exception as e:
        print(f"  âœ— å‚è€ƒåºåˆ—æå–å¤±è´¥: {e}")
        return None
    
    print(f"  å‚è€ƒåºåˆ—é•¿åº¦: {ref_len}")
    
    # 2. è¯»å–å˜å¼‚ä¿¡æ¯ï¼ˆä»…ç”¨äºç»Ÿè®¡æ˜¾ç¤ºï¼‰
    variants = load_variants_in_region_from_vcf(vcf_file, chrom, start, end)
    print(f"  VCFä¸­å˜å¼‚æ•°: {len(variants)}")
    
    if len(variants) > 0:
        n_ins = (variants['variant_type'] == 'INS').sum()
        n_del = (variants['variant_type'] == 'DEL').sum()
        n_snp = (variants['variant_type'] == 'SNP').sum()
        print(f"    (ç»Ÿè®¡: SNP: {n_snp}, INS: {n_ins}, DEL: {n_del})")
    
    # 3. è¯»å–åŸå§‹attentionåˆ†æ•°
    with open(json_file, 'r') as f:
        attention_data = json.load(f)
    
    # 4. è¯»å–æ ·æœ¬åºåˆ—(ç”¨äºéªŒè¯)
    with open(seq_json_file, 'r') as f:
        seq_data = json.load(f)
    
    # æ„å»ºæ ·æœ¬åˆ°åºåˆ—çš„æ˜ å°„
    seq_dict = {item['spec']: item['sequence'] for item in seq_data}
    
    # 5. é€æ ·æœ¬æ ‡å‡†åŒ–
    normalized_data = []
    
    for item in tqdm(attention_data, desc=f"  {block_name} æ ‡å‡†åŒ–", leave=False):
        sample_id = item['spec']
        original_scores = item['sequence']
        
        # è·å–è¯¥æ ·æœ¬çš„åºåˆ—
        if sample_id not in seq_dict:
            print(f"  âš ï¸  æ ·æœ¬ {sample_id} æœªæ‰¾åˆ°åºåˆ—,è·³è¿‡")
            continue
        
        sample_seq = seq_dict[sample_id]
        
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(original_scores) != len(sample_seq):
            print(f"  âš ï¸  æ ·æœ¬ {sample_id} åˆ†æ•°é•¿åº¦({len(original_scores)}) != åºåˆ—é•¿åº¦({len(sample_seq)})")
            # å°è¯•æˆªæ–­æˆ–å¡«å……
            if len(original_scores) > len(sample_seq):
                original_scores = original_scores[:len(sample_seq)]
            else:
                original_scores = original_scores + [0.0] * (len(sample_seq) - len(original_scores))
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä»VCFè¯»å–è¯¥æ ·æœ¬å®é™…æºå¸¦çš„å˜å¼‚
        sample_variants = get_sample_variants_from_vcf(
            vcf_file, chrom, start, end, sample_id, all_samples
        )
        
        # æ„å»ºä½ç½®æ˜ å°„ï¼ˆåªä½¿ç”¨è¯¥æ ·æœ¬å®é™…çš„æ’å…¥å˜å¼‚ï¼‰
        mapping = build_position_mapping_with_sample_variants(
            ref_seq, sample_variants, start
        )
        # ğŸ” å®‰å…¨æ ¡éªŒï¼šæ ·æœ¬é•¿åº¦ â‰  mapping é•¿åº¦ â†’ ç¦æ­¢ä½¿ç”¨æ’å…¥æ˜ å°„
        if len(mapping) != len(sample_seq):
            print(
                f"âš ï¸ å›é€€ä¸ºæ— æ’å…¥æ˜ å°„: {sample_id} "
                f"(seq={len(sample_seq)}, map={len(mapping)})"
            )
            mapping = list(range(len(ref_seq)))



        # æ ‡å‡†åŒ–åˆ†æ•°
        normalized_scores = normalize_attention_scores(original_scores, mapping, ref_len)
        
        normalized_data.append({
            'label': item['label'],
            'spec': sample_id,
            'loc': block_name,
            'sequence': normalized_scores
        })
    
    # 6. ä¿å­˜ç»“æœ
    output_file = os.path.join(output_dir, f"{block_name}_normalized.json")
    with open(output_file, 'w') as f:
        json.dump(normalized_data, f, indent=2)
    
    print(f"  âœ… å®Œæˆ | æ ·æœ¬æ•°: {len(normalized_data)}, æ ‡å‡†åŒ–é•¿åº¦: {ref_len}")
    
    return {
        'block': block_name,
        'samples': len(normalized_data),
        'ref_len': ref_len,
        'output': output_file
    }


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("="*60)
    print("Attentionåˆ†æ•°é•¿åº¦æ ‡å‡†åŒ–ï¼ˆæ”¯æŒå¤šç­‰ä½åŸºå› ï¼‰")
    print("="*60)
    print(f"åŸå§‹JSONç›®å½•: {args.json_dir}")
    print(f"åºåˆ—JSONç›®å½•: {args.seq_json_dir}")
    print(f"VCFæ–‡ä»¶: {args.vcf_file}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # è¯»å–BED
    bed_df = pd.read_csv(args.bed_file, sep="\t", header=None, 
                        names=["chrom", "start", "end"])
    print(f"\nBEDåŒºé—´æ•°: {len(bed_df)}")
    
    # æ‰“å¼€FASTA
    fasta = pysam.FastaFile(args.fasta_file)
    
    # æ£€æŸ¥VCFæ–‡ä»¶
    if not os.path.exists(args.vcf_file):
        print(f"é”™è¯¯: VCFæ–‡ä»¶ä¸å­˜åœ¨: {args.vcf_file}")
        return
    
    # è¯»å–VCFæ ·æœ¬åˆ—è¡¨
    print("\nè¯»å–VCFæ ·æœ¬åˆ—è¡¨...")
    vcf = VCF(args.vcf_file)
    all_samples = vcf.samples
    vcf.close()
    print(f"VCFåŒ…å« {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # å¤„ç†æ¯ä¸ªblock
    results = []
    
    for block_id, row in bed_df.iterrows():
        block_name = f"block_{block_id + 1}"
        
        json_file = os.path.join(args.json_dir, f"{block_name}_attn.json")
        seq_json_file = os.path.join(args.seq_json_dir, f"{block_name}.json")
        
        if not os.path.exists(json_file):
            print(f"\nâš ï¸  è·³è¿‡ {block_name}: æœªæ‰¾åˆ° {json_file}")
            continue
        
        if not os.path.exists(seq_json_file):
            print(f"\nâš ï¸  è·³è¿‡ {block_name}: æœªæ‰¾åˆ° {seq_json_file}")
            continue
        
        result = process_block(
            block_name, json_file, seq_json_file, row,
            args.vcf_file, fasta, args.output_dir, all_samples
        )
        
        if result:
            results.append(result)
    
    # å…³é—­æ–‡ä»¶
    fasta.close()
    
    # ç”Ÿæˆæ€»ç»“
    print("\n" + "="*60)
    print("å¤„ç†å®Œæˆ!")
    print("="*60)
    
    if len(results) > 0:
        summary = pd.DataFrame(results)
        summary_file = os.path.join(args.output_dir, "normalization_summary.csv")
        summary.to_csv(summary_file, index=False)
        
        print(f"\næ€»è®¡å¤„ç†: {len(results)} ä¸ªblocks")
        print(f"æ€»ç»“æ–‡ä»¶: {summary_file}")
    
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")


if __name__ == '__main__':
    main()
