import torch
import json
import os
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

def parse_arguments():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç†JSONæ–‡ä»¶ï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶æ›¿æ¢sequenceå­—æ®µ")
    
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_dir", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶ç›®å½•è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"], help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--file_pattern", type=str, default="*.json", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼")
    parser.add_argument("--output_suffix", type=str, default="_processed", help="è¾“å‡ºæ–‡ä»¶åç¼€")
    parser.add_argument("--keep_original_sequence", action="store_true", help="ä¿ç•™åŸå§‹sequenceå­—æ®µ")
    parser.add_argument("--original_seq_field", type=str, default="original_sequence", help="åŸå§‹åºåˆ—å­—æ®µå")
    parser.add_argument("--debug", action="store_true", help="å¼€å¯è¯¦ç»†è°ƒè¯•ä¿¡æ¯")
    parser.add_argument("--debug_samples", type=int, default=3, help="è¯¦ç»†æ‰“å°å‰Nä¸ªæ ·æœ¬çš„å¤„ç†è¿‡ç¨‹")
    
    return parser.parse_args()

def print_debug(msg, level="INFO", debug_mode=True):
    if debug_mode:
        print(f"[{level}] {msg}")

def main():
    args = parse_arguments()
    
    print("=" * 80)
    print("æ³¨æ„åŠ›åˆ†æ•°æå–æµç¨‹")
    print("=" * 80)
    
    for key, value in vars(args).items():
        print(f"{key}: {value}")
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è·å–JSONæ–‡ä»¶
    input_path = Path(args.input_dir)
    json_files = list(input_path.glob(args.file_pattern))
    
    if not json_files:
        print(f"âŒ é”™è¯¯: åœ¨ {args.input_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
    
    captured_attentions = {}

    def get_attention_hook(name: str):
        def hook(module, inputs, outputs):
            if isinstance(outputs, tuple) and len(outputs) == 2:
                attn_weights = outputs[1]  # [B, H, L, L]
                captured_attentions[name] = attn_weights.detach().cpu()
                if args.debug:
                    print_debug(f"ğŸ¯ æ•è·æ³¨æ„åŠ›æƒé‡: shape={attn_weights.shape}")
        return hook

    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    device = torch.device(args.device)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        attn_implementation="eager",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
    )
    model.to(device)
    model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ, å±‚æ•°: {len(model.model.layers)}")

    # æ³¨å†Œæœ€åä¸€å±‚æ³¨æ„åŠ›é’©å­
    target_layer = model.model.layers[-1].self_attn
    hook_handle = target_layer.register_forward_hook(get_attention_hook("last_self_attn"))
    print(f"âœ… æ³¨æ„åŠ›é’©å­å·²æ³¨å†Œåˆ°æœ€åä¸€å±‚ (layer {len(model.model.layers)-1})")

    def process_single_with_debug(sample, device, sample_idx=0, is_debug=False):
        ref_seq = sample["sequence"]
        
        if is_debug:
            print("\n" + "="*80)
            print(f"ğŸ” å¤„ç†ç¬¬ {sample_idx+1} ä¸ªæ ·æœ¬")
            print(f"ğŸ“ åŸå§‹åºåˆ—é•¿åº¦: {len(ref_seq)}")
            print(f"åºåˆ—å‰50ä¸ªå­—ç¬¦: {ref_seq[:50]}...")
        
        inputs = tokenizer(ref_seq, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        attn_weights = captured_attentions["last_self_attn"]
        attn_weights = attn_weights.float()  # âœ… ç»Ÿä¸€è½¬æ¢ä¸ºfloat32

        if is_debug:
            print(f"ï¿½ï¿½ åŸå§‹æ³¨æ„åŠ›çŸ©é˜µ shape: {attn_weights.shape}")
            # å±•ç¤ºç¬¬ä¸€ä¸ªå¤´çš„å‰10x10çŸ©é˜µ
            first_head = attn_weights[0, 0, :10, :10].numpy()
            print("ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´å‰10x10çŸ©é˜µ:")
            print(first_head)
        
        # å¹³å‡æ‰€æœ‰head
        attn_avg_heads = attn_weights.mean(dim=1)  # [B, L, L]
        
        # å¯¹æ¯ä¸ªtokenæ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªtokençš„â€œé‡è¦æ€§â€
        ref_attn = attn_avg_heads[0].sum(dim=0)  # [L]
        
        new_sample = sample.copy()
        if args.keep_original_sequence:
            new_sample[args.original_seq_field] = sample["sequence"]
        new_sample["sequence"] = ref_attn.cpu().numpy().tolist()
        
        return new_sample

    def process_batch(samples, device):
        sequences = [s["sequence"] for s in samples]
        inputs = tokenizer(sequences, return_tensors="pt", padding=True, truncation=True, max_length=8192)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        batch_attn = captured_attentions["last_self_attn"].float()
        processed_samples = []
        for i, sample in enumerate(samples):
            attn_weights = batch_attn[i:i+1]
            ref_attn = attn_weights.mean(dim=1)[0].sum(dim=0)
            new_sample = sample.copy()
            if args.keep_original_sequence:
                new_sample[args.original_seq_field] = sample["sequence"]
            new_sample["sequence"] = ref_attn.cpu().numpy().tolist()
            processed_samples.append(new_sample)
        return processed_samples

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    total_samples = 0
    processed_files = 0

    for file_idx, json_file in enumerate(json_files):
        output_file = Path(args.output_dir) / f"{json_file.stem}{args.output_suffix}.json"
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶ [{file_idx+1}/{len(json_files)}]: {json_file.name}")

        dataset = load_dataset("json", data_files=str(json_file), split="all")
        all_samples = list(dataset)
        print(f"âœ… åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(all_samples)} ä¸ªæ ·æœ¬")
        
        processed_samples = []
        if args.batch_size > 1:
            for i in tqdm(range(0, len(all_samples), args.batch_size), desc="æ‰¹æ¬¡å¤„ç†"):
                batch = all_samples[i:i+args.batch_size]
                processed_samples.extend(process_batch(batch, device))
                captured_attentions.clear()
        else:
            for i, sample in enumerate(tqdm(all_samples, desc="å¤„ç†æ ·æœ¬")):
                is_debug = args.debug and i < args.debug_samples
                processed_samples.append(process_single_with_debug(sample, device, i, is_debug))
                captured_attentions.clear()

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed_samples, f, ensure_ascii=False, indent=2)
        
        total_samples += len(processed_samples)
        processed_files += 1
        print(f"âœ… å®Œæˆ! å¤„ç† {len(processed_samples)} ä¸ªæ ·æœ¬")

    hook_handle.remove()
    print(f"\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ! å…±å¤„ç† {total_samples} ä¸ªæ ·æœ¬, è¾“å‡º {processed_files} ä¸ªæ–‡ä»¶")

    summary = {
        "total_files_found": len(json_files),
        "files_processed": processed_files,
        "total_samples_processed": total_samples,
        "batch_size": args.batch_size,
        "device": args.device
    }

    summary_file = Path(args.output_dir) / "processing_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“Š å¤„ç†æ‘˜è¦ä¿å­˜åˆ°: {summary_file}")


if __name__ == "__main__":
    main()
