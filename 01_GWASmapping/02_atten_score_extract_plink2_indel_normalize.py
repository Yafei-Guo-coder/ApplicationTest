#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ ‡å‡†åŒ–attentionåˆ†æ•°é•¿åº¦
å°†å› indelå¯¼è‡´çš„ä¸åŒé•¿åº¦åºåˆ—çš„åˆ†æ•°æ ‡å‡†åŒ–ä¸ºå‚è€ƒåºåˆ—é•¿åº¦
- æ’å…¥(INS): å¤šä¸ªç¢±åŸºçš„åˆ†æ•°å¹³å‡ä¸º1ä¸ª
- ç¼ºå¤±(DEL): ä¿æŒNä½ç½®çš„åˆ†æ•°
"""

import os
import json
import numpy as np
import pandas as pd
import pysam
from pathlib import Path
from tqdm import tqdm
import argparse

def parse_args():
    parser = argparse.ArgumentParser(description='æ ‡å‡†åŒ–attentionåˆ†æ•°é•¿åº¦')
    parser.add_argument('--json_dir', type=str, required=True,
                       help='åŒ…å«åŸå§‹JSONæ–‡ä»¶çš„ç›®å½• (block_*.json)')
    parser.add_argument('--seq_json_dir', type=str, required=True,
                       help='åŒ…å«åºåˆ—JSONçš„ç›®å½• (ç”¨äºè·å–å®é™…åºåˆ—)')
    parser.add_argument('--bed_file', type=str, required=True,
                       help='BEDæ–‡ä»¶,å®šä¹‰å‚è€ƒåŒºé—´')
    parser.add_argument('--pvar_prefix', type=str, required=True,
                       help='PLINK2 pvaræ–‡ä»¶å‰ç¼€')
    parser.add_argument('--fasta_file', type=str, required=True,
                       help='å‚è€ƒåŸºå› ç»„FASTA')
    parser.add_argument('--output_dir', type=str, required=True,
                       help='è¾“å‡ºç›®å½•')
    return parser.parse_args()


# æŸ“è‰²ä½“æ˜ å°„
chrom_map = {
    '1': 'AP014957.1', '2': 'AP014958.1', '3': 'AP014959.1',
    '4': 'AP014960.1', '5': 'AP014961.1', '6': 'AP014962.1',
    '7': 'AP014963.1', '8': 'AP014964.1', '9': 'AP014965.1',
    '10': 'AP014966.1', '11': 'AP014967.1', '12': 'AP014968.1'
}


def load_variants_in_region(pvar_file, chrom, start, end):
    """
    è¯»å–PVARæ–‡ä»¶,è·å–æŒ‡å®šåŒºé—´çš„å˜å¼‚ä¿¡æ¯
    
    è¿”å›: DataFrame with columns [POS, REF, ALT, variant_type]
    """
    # è®¡ç®—éœ€è¦è·³è¿‡çš„å…ƒæ•°æ®è¡Œ
    skip_rows = 0
    with open(pvar_file, 'r') as f:
        for line in f:
            if line.startswith('##'):
                skip_rows += 1
            else:
                break
    
    # è¯»å–PVAR
    pvar = pd.read_csv(pvar_file, sep="\t", skiprows=skip_rows)
    pvar.columns = pvar.columns.str.replace('#', '')
    
    # ç­›é€‰è¯¥åŒºé—´çš„å˜å¼‚
    mask = (
        (pvar['CHROM'].astype(str) == str(chrom)) &
        (pvar['POS'] >= start + 1) &
        (pvar['POS'] <= end)
    )
    
    variants = pvar[mask].copy()
    
    # åˆ¤æ–­å˜å¼‚ç±»å‹
    def classify_variant(ref, alt):
        # åªçœ‹ç¬¬ä¸€ä¸ªALT
        alt = str(alt).split(',')[0]
        # è¿‡æ»¤ç‰¹æ®Šæ ‡è®°
        if alt in ['DEL', '<DEL>', '*']:
            return 'DEL'
        
        ref = str(ref).upper()
        alt = alt.upper()
        
        if len(ref) == 1 and len(alt) == 1:
            return 'SNP'
        elif len(ref) < len(alt):
            return 'INS'
        elif len(ref) > len(alt):
            return 'DEL'
        else:
            return 'COMPLEX'
    
    variants['variant_type'] = variants.apply(
        lambda row: classify_variant(row['REF'], row['ALT']), 
        axis=1
    )
    
    # è®¡ç®—æ’å…¥é•¿åº¦
    def get_ins_length(ref, alt):
        alt = str(alt).split(',')[0]
        if alt in ['DEL', '<DEL>', '*']:
            return 0
        return max(0, len(alt) - len(ref))
    
    variants['ins_length'] = variants.apply(
        lambda row: get_ins_length(row['REF'], row['ALT']),
        axis=1
    )
    
    return variants[['POS', 'REF', 'ALT', 'variant_type', 'ins_length']].sort_values('POS')


def build_position_mapping(ref_seq, sample_seq, variants, ref_start):
    """
    æ„å»ºæ ·æœ¬åºåˆ—ä½ç½®åˆ°å‚è€ƒåºåˆ—ä½ç½®çš„æ˜ å°„
    
    å‚æ•°:
        ref_seq: å‚è€ƒåºåˆ—
        sample_seq: æ ·æœ¬åºåˆ—
        variants: è¯¥åŒºé—´çš„å˜å¼‚ä¿¡æ¯ DataFrame
        ref_start: å‚è€ƒåºåˆ—èµ·å§‹ä½ç½®(0-based in genome)
    
    è¿”å›:
        mapping: list, mapping[sample_pos] = ref_pos
                å¦‚æœmapping[i] = j, è¡¨ç¤ºæ ·æœ¬åºåˆ—ç¬¬iä¸ªç¢±åŸºå¯¹åº”å‚è€ƒåºåˆ—ç¬¬jä¸ªç¢±åŸº
                å¦‚æœmapping[i] = -1, è¡¨ç¤ºè¿™æ˜¯æ’å…¥çš„ç¢±åŸº,éœ€è¦ä¸å‰åå¹³å‡
    """
    ref_len = len(ref_seq)
    sample_len = len(sample_seq)
    
    # åˆå§‹åŒ–æ˜ å°„: é»˜è®¤ä¸€ä¸€å¯¹åº”
    mapping = list(range(ref_len))
    
    # å¦‚æœé•¿åº¦ç›¸åŒ,ç›´æ¥è¿”å›
    if sample_len == ref_len:
        return mapping
    
    # æ„å»ºæ¯ä¸ªå‚è€ƒä½ç½®çš„å˜å¼‚ä¿¡æ¯
    var_dict = {}  # {ref_offset: variant_info}
    
    for _, var in variants.iterrows():
        ref_offset = var['POS'] - ref_start - 1  # 0-based offset
        if 0 <= ref_offset < ref_len:
            var_dict[ref_offset] = {
                'type': var['variant_type'],
                'ins_len': var['ins_length']
            }
    
    # é‡å»ºæ˜ å°„
    new_mapping = []
    sample_pos = 0
    ref_pos = 0
    
    while ref_pos < ref_len and sample_pos < sample_len:
        # æ£€æŸ¥è¯¥ä½ç½®æ˜¯å¦æœ‰å˜å¼‚
        if ref_pos in var_dict:
            var_info = var_dict[ref_pos]
            
            if var_info['type'] == 'INS' and var_info['ins_len'] > 0:
                # æ’å…¥: 
                # ç¬¬1ä¸ªç¢±åŸºå¯¹åº”ref_pos
                new_mapping.append(ref_pos)
                sample_pos += 1
                
                # åç»­æ’å…¥çš„ç¢±åŸºæ ‡è®°ä¸º-1(éœ€è¦å¹³å‡)
                for _ in range(var_info['ins_len']):
                    if sample_pos < sample_len:
                        new_mapping.append(-1)  # æ’å…¥æ ‡è®°
                        sample_pos += 1
                
                ref_pos += 1
            
            elif var_info['type'] == 'DEL':
                # ç¼ºå¤±: æ ·æœ¬åºåˆ—è¯¥ä½ç½®æ˜¯N,ä¿æŒå¯¹åº”
                new_mapping.append(ref_pos)
                sample_pos += 1
                ref_pos += 1
            
            else:
                # SNPæˆ–å…¶ä»–
                new_mapping.append(ref_pos)
                sample_pos += 1
                ref_pos += 1
        else:
            # æ— å˜å¼‚,æ­£å¸¸å¯¹åº”
            new_mapping.append(ref_pos)
            sample_pos += 1
            ref_pos += 1
    
    # å¤„ç†å‰©ä½™éƒ¨åˆ†
    while sample_pos < sample_len:
        new_mapping.append(ref_len - 1)  # è¶…å‡ºéƒ¨åˆ†æ˜ å°„åˆ°æœ€å
        sample_pos += 1
    
    return new_mapping


def normalize_attention_scores(scores, mapping, ref_len):
    """
    æ ¹æ®æ˜ å°„å…³ç³»æ ‡å‡†åŒ–attentionåˆ†æ•°
    
    å‚æ•°:
        scores: åŸå§‹åˆ†æ•°åˆ—è¡¨ (é•¿åº¦å¯èƒ½ä¸ç­‰äºref_len)
        mapping: ä½ç½®æ˜ å°„ (ç”±build_position_mappingç”Ÿæˆ)
        ref_len: å‚è€ƒåºåˆ—é•¿åº¦
    
    è¿”å›:
        normalized_scores: æ ‡å‡†åŒ–åçš„åˆ†æ•°åˆ—è¡¨ (é•¿åº¦=ref_len)
    """
    if len(scores) == ref_len and len(mapping) == ref_len:
        # é•¿åº¦å·²ç»ä¸€è‡´,ç›´æ¥è¿”å›
        return scores
    
    # åˆå§‹åŒ–è¾“å‡º
    normalized = [0.0] * ref_len
    counts = [0] * ref_len  # è®°å½•æ¯ä¸ªä½ç½®ç´¯ç§¯äº†å¤šå°‘ä¸ªåˆ†æ•°
    
    # éå†æ ·æœ¬åºåˆ—çš„æ¯ä¸ªä½ç½®
    for sample_pos, score in enumerate(scores):
        if sample_pos >= len(mapping):
            break
        
        ref_pos = mapping[sample_pos]
        
        if ref_pos == -1:
            # è¿™æ˜¯æ’å…¥çš„ç¢±åŸº,éœ€è¦å¹³å‡åˆ°å‰ä¸€ä¸ªä½ç½®
            # æ‰¾åˆ°å‰ä¸€ä¸ªéæ’å…¥ä½ç½®
            prev_pos = sample_pos - 1
            while prev_pos >= 0 and mapping[prev_pos] == -1:
                prev_pos -= 1
            
            if prev_pos >= 0:
                target_ref_pos = mapping[prev_pos]
                if 0 <= target_ref_pos < ref_len:
                    normalized[target_ref_pos] += score
                    counts[target_ref_pos] += 1
        else:
            # æ­£å¸¸ä½ç½®
            if 0 <= ref_pos < ref_len:
                normalized[ref_pos] += float(score)
                counts[ref_pos] += 1
    
    # è®¡ç®—å¹³å‡å€¼
    for i in range(ref_len):
        if counts[i] > 0:
            normalized[i] /= counts[i]
        # å¦‚æœcounts[i]=0,ä¿æŒ0.0
    
    return normalized


def process_block(block_name, json_file, seq_json_file, bed_row, 
                  pvar_file, fasta, output_dir):
    """
    å¤„ç†å•ä¸ªblock
    """
    chrom = str(bed_row['chrom'])
    start = int(bed_row['start'])
    end = int(bed_row['end'])
    
    print(f"\nğŸ“ {block_name}: chr{chrom}:{start}-{end}")
    
    # 1. è¯»å–å‚è€ƒåºåˆ—
    try:
        ref_seq = fasta.fetch(chrom_map[chrom], start, end).upper()
        ref_len = len(ref_seq)
    except Exception as e:
        print(f"  âœ— å‚è€ƒåºåˆ—æå–å¤±è´¥: {e}")
        return None
    
    print(f"  å‚è€ƒåºåˆ—é•¿åº¦: {ref_len}")
    
    # 2. è¯»å–å˜å¼‚ä¿¡æ¯
    variants = load_variants_in_region(pvar_file, chrom, start, end)
    print(f"  å˜å¼‚æ•°: {len(variants)}")
    
    n_ins = (variants['variant_type'] == 'INS').sum()
    n_del = (variants['variant_type'] == 'DEL').sum()
    n_snp = (variants['variant_type'] == 'SNP').sum()
    print(f"    SNP: {n_snp}, INS: {n_ins}, DEL: {n_del}")
    
    # 3. è¯»å–åŸå§‹attentionåˆ†æ•°
    with open(json_file, 'r') as f:
        attention_data = json.load(f)
    
    # 4. è¯»å–æ ·æœ¬åºåˆ—(ç”¨äºæ„å»ºæ˜ å°„)
    with open(seq_json_file, 'r') as f:
        seq_data = json.load(f)
    
    # æ„å»ºæ ·æœ¬åˆ°åºåˆ—çš„æ˜ å°„
    seq_dict = {item['spec']: item['sequence'] for item in seq_data}
    
    # 5. é€æ ·æœ¬æ ‡å‡†åŒ–
    normalized_data = []
    
    for item in tqdm(attention_data, desc=f"  {block_name} æ ‡å‡†åŒ–", leave=False):
        sample_id = item['spec']
        original_scores = item['sequence']
        
        # è·å–è¯¥æ ·æœ¬çš„åºåˆ—
        if sample_id not in seq_dict:
            print(f"  âš ï¸  æ ·æœ¬ {sample_id} æœªæ‰¾åˆ°åºåˆ—,è·³è¿‡")
            continue
        
        sample_seq = seq_dict[sample_id]
        
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
        if len(original_scores) != len(sample_seq):
            print(f"  âš ï¸  æ ·æœ¬ {sample_id} åˆ†æ•°é•¿åº¦({len(original_scores)}) != åºåˆ—é•¿åº¦({len(sample_seq)})")
            # å°è¯•æˆªæ–­æˆ–å¡«å……
            if len(original_scores) > len(sample_seq):
                original_scores = original_scores[:len(sample_seq)]
            else:
                original_scores = original_scores + [0.0] * (len(sample_seq) - len(original_scores))
        
        # æ„å»ºä½ç½®æ˜ å°„
        mapping = build_position_mapping(ref_seq, sample_seq, variants, start)
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        normalized_scores = normalize_attention_scores(original_scores, mapping, ref_len)
        
        normalized_data.append({
            'label': item['label'],
            'spec': sample_id,
            'loc': block_name,
            'sequence': normalized_scores
        })
    
    # 6. ä¿å­˜ç»“æœ
    output_file = os.path.join(output_dir, f"{block_name}_processed.json")
    with open(output_file, 'w') as f:
        json.dump(normalized_data, f, indent=2)
    
    print(f"  âœ… å®Œæˆ | æ ·æœ¬æ•°: {len(normalized_data)}, æ ‡å‡†åŒ–é•¿åº¦: {ref_len}")
    
    return {
        'block': block_name,
        'samples': len(normalized_data),
        'ref_len': ref_len,
        'output': output_file
    }


def main():
    args = parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("="*60)
    print("Attentionåˆ†æ•°é•¿åº¦æ ‡å‡†åŒ–")
    print("="*60)
    print(f"åŸå§‹JSONç›®å½•: {args.json_dir}")
    print(f"åºåˆ—JSONç›®å½•: {args.seq_json_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # è¯»å–BED
    bed_df = pd.read_csv(args.bed_file, sep="\t", header=None, 
                        names=["chrom", "start", "end"])
    print(f"\nBEDåŒºé—´æ•°: {len(bed_df)}")
    
    # æ‰“å¼€FASTA
    fasta = pysam.FastaFile(args.fasta_file)
    
    # PVARæ–‡ä»¶
    pvar_file = f"{args.pvar_prefix}.pvar"
    if not os.path.exists(pvar_file):
        print(f"é”™è¯¯: PVARæ–‡ä»¶ä¸å­˜åœ¨: {pvar_file}")
        return
    
    # å¤„ç†æ¯ä¸ªblock
    results = []
    
    for block_id, row in bed_df.iterrows():
        block_name = f"block_{block_id + 1}"
        
        json_file = os.path.join(args.json_dir, f"{block_name}_processed.json")
        seq_json_file = os.path.join(args.seq_json_dir, f"{block_name}.json")
        
        if not os.path.exists(json_file):
            print(f"\nâš ï¸  è·³è¿‡ {block_name}: æœªæ‰¾åˆ° {json_file}")
            continue
        
        if not os.path.exists(seq_json_file):
            print(f"\nâš ï¸  è·³è¿‡ {block_name}: æœªæ‰¾åˆ° {seq_json_file}")
            continue
        
        result = process_block(
            block_name, json_file, seq_json_file, row,
            pvar_file, fasta, args.output_dir
        )
        
        if result:
            results.append(result)
    
    # å…³é—­æ–‡ä»¶
    fasta.close()
    
    # ç”Ÿæˆæ€»ç»“
    print("\n" + "="*60)
    print("å¤„ç†å®Œæˆ!")
    print("="*60)
    
    summary = pd.DataFrame(results)
    summary_file = os.path.join(args.output_dir, "normalization_summary.csv")
    summary.to_csv(summary_file, index=False)
    
    print(f"\næ€»è®¡å¤„ç†: {len(results)} ä¸ªblocks")
    print(f"æ€»ç»“æ–‡ä»¶: {summary_file}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")


if __name__ == '__main__':
    main()
