import torch
import json
import os
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from tqdm import tqdm

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç†JSONæ–‡ä»¶ï¼Œè®¡ç®—æ³¨æ„åŠ›æƒé‡å¹¶æ›¿æ¢sequenceå­—æ®µ")
    
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_dir", type=str, required=True, help="è¾“å…¥JSONæ–‡ä»¶ç›®å½•è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True, help="è¾“å‡ºJSONæ–‡ä»¶ç›®å½•è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤ä¸º1")
    parser.add_argument("--device", type=str, default="cuda", choices=["cuda", "cpu"], help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--file_pattern", type=str, default="*.json", help="æ–‡ä»¶åŒ¹é…æ¨¡å¼")
    parser.add_argument("--output_suffix", type=str, default="_processed", help="è¾“å‡ºæ–‡ä»¶åç¼€")
    parser.add_argument("--keep_original_sequence", action="store_true", help="ä¿ç•™åŸå§‹sequenceå­—æ®µ")
    parser.add_argument("--original_seq_field", type=str, default="original_sequence", help="åŸå§‹åºåˆ—å­—æ®µå")
    parser.add_argument("--debug", action="store_true", help="å¼€å¯è¯¦ç»†è°ƒè¯•ä¿¡æ¯")
    parser.add_argument("--debug_samples", type=int, default=3, help="è¯¦ç»†æ‰“å°å‰Nä¸ªæ ·æœ¬çš„å¤„ç†è¿‡ç¨‹")
    
    return parser.parse_args()

def print_debug(msg, level="INFO", debug_mode=True):
    """æ‰“å°è°ƒè¯•ä¿¡æ¯"""
    if debug_mode:
        print(f"[{level}] {msg}")

def main():
    args = parse_arguments()
    
    print("=" * 80)
    print("æ³¨æ„åŠ›åˆ†æ•°æå–æµç¨‹")
    print("=" * 80)
    print("å‚æ•°é…ç½®:")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("=" * 80)
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    input_path = Path(args.input_dir)
    json_files = list(input_path.glob(args.file_pattern))
    
    if not json_files:
        print(f"âŒ é”™è¯¯: åœ¨ {args.input_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶")
        return
    
    print(f"\nâœ… æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
    
    # å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æ³¨æ„åŠ›æƒé‡
    captured_attentions = {}
    
    def get_attention_hook(name: str):
        """è¿”å›ä¸€ä¸ª hook å‡½æ•°ï¼Œç”¨äºæ•è·æŒ‡å®šæ¨¡å—çš„è¾“å‡º"""
        def hook(module, inputs, outputs):
            if isinstance(outputs, tuple) and len(outputs) == 2:
                attn_weights = outputs[1]  # [B, H, L, L]
                captured_attentions[name] = attn_weights.detach().cpu()
                if args.debug:
                    print_debug(f"  ğŸ¯ æ•è·æ³¨æ„åŠ›æƒé‡: shape={attn_weights.shape}")
        return hook
    
    # åŠ è½½æ¨¡å‹
    print("\n" + "="*80)
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
        device = torch.device(args.device)
        
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            attn_implementation="eager",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
        )
        model.to(device)
        model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   è®¾å¤‡: {device}")
        print(f"   æ¨¡å‹å±‚æ•°: {len(model.model.layers)}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        return
    
    # æ³¨å†Œé’©å­ï¼ˆæ•è·æœ€åä¸€å±‚çš„æ³¨æ„åŠ›ï¼‰
    try:
        target_layer = model.model.layers[-1].self_attn
        hook_handle = target_layer.register_forward_hook(get_attention_hook("last_self_attn"))
        print(f"âœ… æ³¨æ„åŠ›é’©å­å·²æ³¨å†Œåˆ°æœ€åä¸€å±‚ (layer {len(model.model.layers)-1})")
    except Exception as e:
        print(f"âŒ æ³¨å†Œé’©å­å¤±è´¥: {e}")
        return
    
    def process_single_with_debug(sample, device, sample_idx=0, is_debug=False):
        """å•æ ·æœ¬å¤„ç†ï¼ˆå¸¦è°ƒè¯•ä¿¡æ¯ï¼‰"""
        ref_seq = sample["sequence"]
        
        if is_debug:
            print("\n" + "="*80)
            print(f"ğŸ” è¯¦ç»†å¤„ç†ç¬¬ {sample_idx+1} ä¸ªæ ·æœ¬")
            print("="*80)
            print(f"ğŸ“ åŸå§‹æ•°æ®:")
            print(f"   æ ·æœ¬ID: {sample.get('spec', 'N/A')}")
            print(f"   æ ‡ç­¾: {sample.get('label', 'N/A')}")
            print(f"   ä½ç½®: {sample.get('loc', 'N/A')}")
            print(f"   åºåˆ—é•¿åº¦: {len(ref_seq)}")
            print(f"   åºåˆ—å‰50ä¸ªå­—ç¬¦: {ref_seq[:50]}...")
        
        # Step 1: Tokenization
        inputs = tokenizer(ref_seq, return_tensors="pt")
        
        if is_debug:
            print(f"\nğŸ“Š Step 1: Tokenization")
            print(f"   input_ids shape: {inputs['input_ids'].shape}")
            print(f"   åºåˆ—è¢«åˆ†æˆ {inputs['input_ids'].shape[1]} ä¸ªtokens")
            print(f"   å‰10ä¸ªtoken IDs: {inputs['input_ids'][0, :10].tolist()}")
        
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Step 2: æ¨¡å‹å‰å‘ä¼ æ’­
        if is_debug:
            print(f"\nğŸ§  Step 2: æ¨¡å‹å‰å‘ä¼ æ’­")
            print(f"   å°†tokensè¾“å…¥æ¨¡å‹...")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        if is_debug:
            print(f"   âœ“ æ¨¡å‹è¾“å‡ºå®Œæˆ")
            print(f"   logits shape: {outputs.logits.shape}")
        
        # Step 3: è·å–æ³¨æ„åŠ›æƒé‡
        attn_weights = captured_attentions["last_self_attn"]
        
        if is_debug:
            print(f"\nğŸ¯ Step 3: æ³¨æ„åŠ›æƒé‡")
            print(f"   åŸå§‹æ³¨æ„åŠ›çŸ©é˜µ shape: {attn_weights.shape}")
            print(f"   è§£é‡Š:")
            print(f"     - ç»´åº¦0 (Batch): {attn_weights.shape[0]} (æ‰¹æ¬¡å¤§å°)")
            print(f"     - ç»´åº¦1 (Heads): {attn_weights.shape[1]} (æ³¨æ„åŠ›å¤´æ•°)")
            print(f"     - ç»´åº¦2 (Query): {attn_weights.shape[2]} (æŸ¥è¯¢åºåˆ—é•¿åº¦)")
            print(f"     - ç»´åº¦3 (Key): {attn_weights.shape[3]} (é”®åºåˆ—é•¿åº¦)")
            print(f"\n   æ³¨æ„åŠ›çŸ©é˜µå«ä¹‰:")
            print(f"     attn[i,j,k,l] = ç¬¬iä¸ªæ ·æœ¬ã€ç¬¬jä¸ªæ³¨æ„åŠ›å¤´ã€")
            print(f"                      ç¬¬kä¸ªtokenå¯¹ç¬¬lä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°")
            
            # ğŸ”¥ æ–°å¢ï¼šå±•ç¤ºå•ä¸ªæ³¨æ„åŠ›å¤´çš„çŸ©é˜µ
            print(f"\n   ğŸ“Š ç¬¬1ä¸ªæ³¨æ„åŠ›å¤´çš„çŸ©é˜µé¢„è§ˆ (å‰10x10):")
            first_head = attn_weights[0, 0, :10, :10].cpu().float().numpy()
            print(f"   è¡Œ=Queryä½ç½®, åˆ—=Keyä½ç½®, å€¼=æ³¨æ„åŠ›åˆ†æ•°")
            print(f"   (æ¯è¡Œä¹‹å’Œ=1.0ï¼Œå› ä¸ºæ˜¯softmaxåçš„æ¦‚ç‡åˆ†å¸ƒ)")
            print()
            print("        ", end="")
            for col in range(10):
                print(f"  K{col:2d}  ", end="")
            print()
            for row in range(10):
                print(f"   Q{row:2d}  ", end="")
                for col in range(10):
                    print(f"{first_head[row, col]:6.3f}", end=" ")
                print()
            
            # ğŸ”¥ éªŒè¯æ¯è¡Œå’Œ
            row_sums = attn_weights[0, 0].sum(dim=1)[:10].cpu().float().numpy()
            print(f"\n   âœ“ éªŒè¯: å‰10è¡Œçš„è¡Œå’Œ (åº”è¯¥éƒ½çº¦ç­‰äº1.0):")
            print(f"   {row_sums}")
            
            # ğŸ”¥ ä¸Šä¸‰è§’ vs ä¸‹ä¸‰è§’åˆ†æ
            print(f"\n   ğŸ”º ä¸Šä¸‰è§’ vs ä¸‹ä¸‰è§’åˆ†æ:")
            print(f"   åœ¨è‡ªå›å½’æ¨¡å‹(å¦‚GPT)ä¸­:")
            print(f"     - ä¸‹ä¸‰è§’ (åŒ…æ‹¬å¯¹è§’çº¿): tokenåªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„token")
            print(f"     - ä¸Šä¸‰è§’: tokençœ‹æœªæ¥çš„token (é€šå¸¸è¢«maskæ‰=0)")
            
            full_matrix = attn_weights[0, 0].cpu().float().numpy()
            # ä¸‹ä¸‰è§’å’Œ (åŒ…æ‹¬å¯¹è§’çº¿)
            import numpy as np
            lower_tri = np.tril(full_matrix)
            upper_tri = np.triu(full_matrix, k=1)  # k=1æ’é™¤å¯¹è§’çº¿
            
            lower_sum = lower_tri.sum()
            upper_sum = upper_tri.sum()
            total_sum = full_matrix.sum()
            
            print(f"     - ä¸‹ä¸‰è§’å’Œ (åŒ…æ‹¬å¯¹è§’çº¿): {lower_sum:.3f} ({lower_sum/total_sum*100:.1f}%)")
            print(f"     - ä¸Šä¸‰è§’å’Œ (ä¸å«å¯¹è§’çº¿): {upper_sum:.3f} ({upper_sum/total_sum*100:.1f}%)")
            print(f"     - æ€»å’Œ: {total_sum:.3f}")
            
            if upper_sum < 0.01:
                print(f"   âœ“ æ£€æµ‹åˆ°å› æœæ³¨æ„åŠ›mask (ä¸Šä¸‰è§’â‰ˆ0)")
            else:
                print(f"   âœ“ åŒå‘æ³¨æ„åŠ› (æ— mask)")
            
            # ğŸ”¥ å¯¹è§’çº¿åˆ†æ
            diagonal = np.diag(full_matrix)
            print(f"\n   ğŸ“ å¯¹è§’çº¿åˆ†æ (tokenå¯¹è‡ªå·±çš„æ³¨æ„åŠ›):")
            print(f"     - å¯¹è§’çº¿å¹³å‡å€¼: {diagonal.mean():.4f}")
            print(f"     - å¯¹è§’çº¿æœ€å¤§å€¼: {diagonal.max():.4f}")
            print(f"     - å¯¹è§’çº¿æœ€å°å€¼: {diagonal.min():.4f}")
            print(f"     - å‰10ä¸ªå¯¹è§’çº¿å€¼: {diagonal[:10]}")
        
        # Step 4: å¹³å‡æ³¨æ„åŠ›å¤´
        attn_avg_heads = attn_weights.mean(dim=1)  # [B, L, L] -> [1, L, L]
        
        if is_debug:
            print(f"\nğŸ“ˆ Step 4: å¹³å‡æ‰€æœ‰æ³¨æ„åŠ›å¤´")
            print(f"   æ“ä½œ: attn_weights.mean(dim=1)")
            print(f"   å¹³å‡å shape: {attn_avg_heads.shape}")
            print(f"   è§£é‡Š: å°† {attn_weights.shape[1]} ä¸ªæ³¨æ„åŠ›å¤´çš„ç»“æœå–å¹³å‡")
            print(f"   ç°åœ¨çŸ©é˜µ[k,l] = ç¬¬kä¸ªtokenå¯¹ç¬¬lä¸ªtokençš„å¹³å‡æ³¨æ„åŠ›")
            
            # ğŸ”¥ å¹³å‡åçš„çŸ©é˜µé¢„è§ˆ
            print(f"\n   ğŸ“Š å¹³å‡åçš„çŸ©é˜µé¢„è§ˆ (å‰10x10):")
            avg_matrix = attn_avg_heads[0, :10, :10].cpu().float().numpy()
            print("        ", end="")
            for col in range(10):
                print(f"  K{col:2d}  ", end="")
            print()
            for row in range(10):
                print(f"   Q{row:2d}  ", end="")
                for col in range(10):
                    print(f"{avg_matrix[row, col]:6.3f}", end=" ")
                print()
        
        # Step 5: å¯¹æ¯ä¸ªtokenæ±‚å’Œï¼ˆè·å¾—æ¯ä¸ªtokenæ¥æ”¶çš„æ€»æ³¨æ„åŠ›ï¼‰
        ref_attn = attn_avg_heads[0].sum(dim=0)  # [L, L] -> [L]
        
        if is_debug:
            print(f"\nâ• Step 5: å¯¹æ¯ä¸ªtokenæ±‚å’Œ")
            print(f"   æ“ä½œ: attn_avg_heads[0].sum(dim=0)")
            print(f"   æ±‚å’Œå shape: {ref_attn.shape}")
            print(f"   è§£é‡Š: å¯¹æ¯ä¸€åˆ—æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªtokenæ¥æ”¶çš„æ€»æ³¨æ„åŠ›")
            print(f"\n   ğŸ“ ç†è§£è¿™ä¸ªæ±‚å’Œ:")
            print(f"   åŸçŸ©é˜µ: æ¯è¡Œä»£è¡¨ä¸€ä¸ªQuery tokençœ‹å…¶ä»–tokençš„æ³¨æ„åŠ›")
            print(f"   åˆ—æ±‚å’Œ: ç»Ÿè®¡æ¯ä¸ªKey tokenè¢«å¤šå°‘ä¸ªQueryå…³æ³¨")
            print(f"   ç»“æœ: ä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªtokençš„'é‡è¦æ€§'")
            
            # ğŸ”¥ æ‰‹åŠ¨å±•ç¤ºç¬¬0åˆ—çš„æ±‚å’Œè¿‡ç¨‹
            col_0_values = attn_avg_heads[0, :, 0].cpu().numpy()
            print(f"\n   ğŸ” ç¤ºä¾‹: ç¬¬0ä¸ªtokençš„æ€»æ³¨æ„åŠ› = ç¬¬0åˆ—ä¹‹å’Œ")
            print(f"   ç¬¬0åˆ—çš„å€¼ (æ‰€æœ‰tokenå¯¹ç¬¬0ä¸ªtokençš„æ³¨æ„åŠ›):")
            print(f"   {col_0_values[:10]}... (å‰10ä¸ª)")
            print(f"   æ±‚å’Œ = {col_0_values.sum():.6f}")
            print(f"   éªŒè¯: ref_attn[0] = {ref_attn[0].item():.6f} âœ“")
            
            print(f"\n   ğŸ¯ æœ€ç»ˆæ³¨æ„åŠ›åˆ†æ•°:")
            print(f"     - å‘é‡é•¿åº¦: {len(ref_attn)}")
            print(f"     - æœ€å°å€¼: {ref_attn.min().item():.6f}")
            print(f"     - æœ€å¤§å€¼: {ref_attn.max().item():.6f}")
            print(f"     - å¹³å‡å€¼: {ref_attn.mean().item():.6f}")
            print(f"     - å‰10ä¸ªå€¼: {ref_attn[:10].tolist()}")
            
            # å¯è§†åŒ–æ³¨æ„åŠ›åˆ†å¸ƒ
            print(f"\n   ğŸ“Š æ³¨æ„åŠ›åˆ†æ•°åˆ†å¸ƒ:")
            scores = ref_attn.cpu().float().numpy()
            import numpy as np
            percentiles = [0, 25, 50, 75, 100]
            for p in percentiles:
                val = np.percentile(scores, p)
                print(f"     {p}th percentile: {val:.6f}")
        
        # Step 6: åˆ›å»ºæ–°æ ·æœ¬
        new_sample = sample.copy()
        
        if args.keep_original_sequence:
            new_sample[args.original_seq_field] = new_sample["sequence"]
        
        # æ›¿æ¢sequenceå­—æ®µä¸ºæ³¨æ„åŠ›åˆ†æ•°
        new_sample["sequence"] = ref_attn.cpu().float().numpy().tolist()
        
        if is_debug:
            print(f"\nâœ… Step 6: ä¿å­˜ç»“æœ")
            print(f"   åŸå§‹åºåˆ—é•¿åº¦: {len(ref_seq)}")
            print(f"   æ³¨æ„åŠ›å‘é‡é•¿åº¦: {len(new_sample['sequence'])}")
            if args.keep_original_sequence:
                print(f"   åŸå§‹åºåˆ—ä¿å­˜åœ¨å­—æ®µ: '{args.original_seq_field}'")
            print("="*80)
        
        return new_sample
    
    def process_batch(samples, device):
        """æ‰¹é‡å¤„ç†æ ·æœ¬"""
        sequences = [sample["sequence"] for sample in samples]
        
        print(f"\n  æ‰¹é‡å¤„ç† {len(sequences)} ä¸ªæ ·æœ¬...")
        
        inputs = tokenizer(
            sequences,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=8192
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        if args.debug:
            print(f"  Tokenized shape: {inputs['input_ids'].shape}")
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        batch_attentions = captured_attentions["last_self_attn"]
        
        if args.debug:
            print(f"  æ‰¹é‡æ³¨æ„åŠ› shape: {batch_attentions.shape}")
        
        processed_samples = []
        
        for i, sample in enumerate(samples):
            attn_weights = batch_attentions[i:i+1]
            
            try:
                ref_attn = attn_weights.mean(dim=1)[0].sum(dim=0)
                new_sample = sample.copy()
                
                if args.keep_original_sequence:
                    new_sample[args.original_seq_field] = new_sample["sequence"]
                
                new_sample["sequence"] = ref_attn.cpu().float().numpy().tolist()
                processed_samples.append(new_sample)
                
            except Exception as e:
                print(f"  âŒ å¤„ç†æ‰¹æ¬¡ä¸­æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                processed_samples.append(sample)
        
        return processed_samples
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    total_samples = 0
    processed_files = 0
    
    for file_idx, json_file in enumerate(json_files):
        output_file = Path(args.output_dir) / f"{json_file.stem}{args.output_suffix}.json"
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶ [{file_idx+1}/{len(json_files)}]: {json_file.name}")
        print(f"ğŸ“¤ è¾“å‡ºåˆ°: {output_file.name}")
        print("="*80)
        
        try:
            dataset = load_dataset("json", data_files=str(json_file), split="all")
            all_samples = list(dataset)
            
            print(f"âœ… åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(all_samples)} ä¸ªæ ·æœ¬")
            
            processed_samples = []
            
            if args.batch_size > 1:
                # æ‰¹é‡å¤„ç†
                for i in tqdm(range(0, len(all_samples), args.batch_size), 
                            desc="æ‰¹æ¬¡å¤„ç†", unit="batch"):
                    batch = all_samples[i:i + args.batch_size]
                    processed_batch = process_batch(batch, device)
                    processed_samples.extend(processed_batch)
                    captured_attentions.clear()
            else:
                # å•æ ·æœ¬å¤„ç†
                for i, sample in enumerate(tqdm(all_samples, desc="å¤„ç†æ ·æœ¬", unit="sample")):
                    # å‰Nä¸ªæ ·æœ¬è¯¦ç»†æ‰“å°
                    is_debug = args.debug and i < args.debug_samples
                    processed_sample = process_single_with_debug(sample, device, i, is_debug)
                    processed_samples.append(processed_sample)
                    captured_attentions.clear()
            
            # ä¿å­˜ç»“æœ
            print(f"\nğŸ’¾ ä¿å­˜å¤„ç†ç»“æœ...")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(processed_samples, f, ensure_ascii=False, indent=2)
            
            samples_processed = len(processed_samples)
            total_samples += samples_processed
            processed_files += 1
            
            print(f"âœ… å®Œæˆ! å¤„ç†äº† {samples_processed} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ¸…ç†
    hook_handle.remove()
    
    print("\n" + "="*80)
    print("ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ!")
    print("="*80)
    print(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶: {processed_files}/{len(json_files)}")
    print(f"âœ… æ€»å…±å¤„ç†æ ·æœ¬: {total_samples}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*80)
    
    # ä¿å­˜æ‘˜è¦
    summary = {
        "total_files_found": len(json_files),
        "files_processed": processed_files,
        "total_samples_processed": total_samples,
        "batch_size": args.batch_size,
        "device": args.device,
    }
    
    summary_file = Path(args.output_dir) / "processing_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š å¤„ç†æ‘˜è¦ä¿å­˜åˆ°: {summary_file}")

if __name__ == "__main__":
    main()
